{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5TokenizerFast, TFAutoModelForSeq2SeqLM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:22:24.887573Z",
     "iopub.status.busy": "2023-07-10T18:22:24.886869Z",
     "iopub.status.idle": "2023-07-10T18:22:24.895410Z",
     "shell.execute_reply": "2023-07-10T18:22:24.894099Z",
     "shell.execute_reply.started": "2023-07-10T18:22:24.887540Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(url: str, n_rows: int):\n",
    "    data = pd.read_csv(url).iloc[:n_rows, 1:]\n",
    "    data = data.sample(frac=1).copy()\n",
    "    data.columns = [\"input\", \"target\"]\n",
    "    data = data.loc[(~data[\"input\"].isna()) & (~data[\"target\"].isna())].copy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:22:24.897970Z",
     "iopub.status.busy": "2023-07-10T18:22:24.897163Z",
     "iopub.status.idle": "2023-07-10T18:22:54.454528Z",
     "shell.execute_reply": "2023-07-10T18:22:54.453312Z",
     "shell.execute_reply.started": "2023-07-10T18:22:24.897927Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_data(url=\"data/train.csv\", n_rows=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:22:54.458870Z",
     "iopub.status.busy": "2023-07-10T18:22:54.458358Z",
     "iopub.status.idle": "2023-07-10T18:22:54.476208Z",
     "shell.execute_reply": "2023-07-10T18:22:54.474942Z",
     "shell.execute_reply.started": "2023-07-10T18:22:54.458833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>JERUSALEM (CNN) -- Clashes between Hamas milit...</td>\n",
       "      <td>NEW: Fighting nears densely populated Gaza Cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>By . Sara Malm . PUBLISHED: . 05:36 EST, 25 Fe...</td>\n",
       "      <td>One died and four were injured at a wedding in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>Seoul, South Korea (CNN) -- North Korea says i...</td>\n",
       "      <td>SK minister: \"The agreement must be kept\"\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>An underground website that was dealing cocain...</td>\n",
       "      <td>Silk Road owner, Ross William Ulbricht, 29, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6359</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 08:32 EST...</td>\n",
       "      <td>Steve Capus announced his resignation on Frida...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "5798  JERUSALEM (CNN) -- Clashes between Hamas milit...   \n",
       "4640  By . Sara Malm . PUBLISHED: . 05:36 EST, 25 Fe...   \n",
       "9924  Seoul, South Korea (CNN) -- North Korea says i...   \n",
       "2451  An underground website that was dealing cocain...   \n",
       "6359  By . Associated Press . PUBLISHED: . 08:32 EST...   \n",
       "\n",
       "                                                 target  \n",
       "5798  NEW: Fighting nears densely populated Gaza Cit...  \n",
       "4640  One died and four were injured at a wedding in...  \n",
       "9924  SK minister: \"The agreement must be kept\"\\nThe...  \n",
       "2451  Silk Road owner, Ross William Ulbricht, 29, kn...  \n",
       "6359  Steve Capus announced his resignation on Frida...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:22:54.478265Z",
     "iopub.status.busy": "2023-07-10T18:22:54.477699Z",
     "iopub.status.idle": "2023-07-10T18:22:54.484105Z",
     "shell.execute_reply": "2023-07-10T18:22:54.483102Z",
     "shell.execute_reply.started": "2023-07-10T18:22:54.478231Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame):\n",
    "    data = data.copy()\n",
    "    data[\"input\"] = \"summarize: \" + data[\"input\"].map(unidecode).copy()\n",
    "    data[\"target\"] = data[\"target\"].map(unidecode).copy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:22:54.485585Z",
     "iopub.status.busy": "2023-07-10T18:22:54.485209Z",
     "iopub.status.idle": "2023-07-10T18:23:23.012576Z",
     "shell.execute_reply": "2023-07-10T18:23:23.011327Z",
     "shell.execute_reply.started": "2023-07-10T18:22:54.485550Z"
    }
   },
   "outputs": [],
   "source": [
    "data = preprocess_data(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:23.015029Z",
     "iopub.status.busy": "2023-07-10T18:23:23.014594Z",
     "iopub.status.idle": "2023-07-10T18:23:23.021992Z",
     "shell.execute_reply": "2023-07-10T18:23:23.020930Z",
     "shell.execute_reply.started": "2023-07-10T18:23:23.014988Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(data: pd.DataFrame, input_col: str=\"input\", target_col: str=\"target\", test_size: float=0.1):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[input_col], data[target_col], \n",
    "                                                        random_state=42, test_size=test_size)\n",
    "    \n",
    "    print(f'x_train.shape: {x_train.shape}, x_test.shape: {x_test.shape}, '+\n",
    "          f'y_train.shape: {y_train.shape}, y_test.shape: {y_test.shape}')\n",
    "    x_train, x_test, y_train, y_test = x_train.to_list(), x_test.to_list(), y_train.to_list(), y_test.to_list()\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:23.024226Z",
     "iopub.status.busy": "2023-07-10T18:23:23.023539Z",
     "iopub.status.idle": "2023-07-10T18:23:23.041781Z",
     "shell.execute_reply": "2023-07-10T18:23:23.040590Z",
     "shell.execute_reply.started": "2023-07-10T18:23:23.024189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (18000,), x_test.shape: (2000,), y_train.shape: (18000,), y_test.shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:23.043645Z",
     "iopub.status.busy": "2023-07-10T18:23:23.043242Z",
     "iopub.status.idle": "2023-07-10T18:23:23.053015Z",
     "shell.execute_reply": "2023-07-10T18:23:23.051804Z",
     "shell.execute_reply.started": "2023-07-10T18:23:23.043587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"summarize: Back in the UK: John Cleese and his wife Jennifer Wade . The crippling cost of divorce has already forced John Cleese into crisis measures, such as launching his stand-up comedy 'Alimony Tour', and even moving to another country. But it would seem he is still looking for ways to keep his finances afloat. The Monty Python star has embarked on a sale of film props and signed photos he accumulated during his career. The items include a fibre-glass helmet used in the film Monty Python And The Holy Grail, which is being offered at PS999. There is also a 1970 photo which shows the Monty Python stars doing silly walks, priced at PS29.99, and a signed photo of the classic Fawlty Towers scene in which Cleese as Basil Fawlty thrashes his red Austin 1300 with a tree branch. The photos are among seven of Cleese performing in Python and Fawlty Towers sketches which, with the other items, are for sale on the Original Memorabilia Company website. It is the same website the 73-year-old actor previously used to sell his beloved 1987 Bentley Eight, which was bought for PS17,100. As part of the deal he agreed to have . lunch with whoever bought it and give a handwritten story of some of the . car's most famous passengers, including Jamie Lee Curtis and Kevin . Kline, his co-stars from the film A Fish Called Wanda. Cleese, . who last year married his fourth wife, Jennifer Wade, 31 years his . junior, divorced American psychotherapist Alyce Faye Eichelberger in . 2008 after 16 years of marriage. He . was ordered to pay PS12.5million in finance and assets, including . PS612,000 a year until 2016 and hand over an apartment in New York, and a . PS2million mews house in Holland Park, West London. Scroll down for video . Sale: The items include a fibre-glass helmet used in the film Monty Python And The Holy Grail, which is being offered at PS999 . Famous: There is also a 1970 photo which shows the Monty Python stars doing silly walks, priced at PS29.99 . Prize car: The Original Memorabilia Company website is the same place that Cleese previously used to sell his beloved 1987 Bentley Eight, which was bought for PS17,100 . Faced with the demands, he was forced to sell his PS6million California beach house. Cleese has also moved from the US to Monaco to avoid paying a huge tax bill on his divorce payments to Miss Eichelberger. He has already said he would not return to the UK because of harsh tax laws. It also prompted him to embark on his stand-up tour, in which he told audiences: 'I am here because, frankly, I have fallen on hard times having been through a costly and acrimonious divorce. I call it the Alimony Tour or Feeding The Beast.' Cleese has remained friends with his first wife Connie Booth, who co-wrote and starred in Fawlty Towers with him, and his second wife, US actress Barbara Trentham.\",\n",
       " 'Python star John Cleese, 73, married fourth wife Jennifer Wade last year .\\nDivorced Alyce Faye Eichelberger in 2008 and ordered to pay her PS12.5m .\\nSelling fibre-glass helmet from Monty Python and the Holy Grail for PS999 .\\nAlso selling photos of silly walks and Fawlty Towers car thrashing scene .')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:23.058197Z",
     "iopub.status.busy": "2023-07-10T18:23:23.057907Z",
     "iopub.status.idle": "2023-07-10T18:23:28.649345Z",
     "shell.execute_reply": "2023-07-10T18:23:28.648189Z",
     "shell.execute_reply.started": "2023-07-10T18:23:23.058173Z"
    }
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "CHECKPOINT = \"t5-small\"\n",
    "INPUT_N_TOKENS = 300 # considering only 300 tokens due to memory constraints\n",
    "TARGET_N_TOKENS = 150 # considering only 150 tokens due to memory constraints\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:28.651912Z",
     "iopub.status.busy": "2023-07-10T18:23:28.650808Z",
     "iopub.status.idle": "2023-07-10T18:23:28.659232Z",
     "shell.execute_reply": "2023-07-10T18:23:28.657968Z",
     "shell.execute_reply.started": "2023-07-10T18:23:28.651871Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(input: list, target: list, input_n_tokens: int, target_n_tokens: int):\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(CHECKPOINT)\n",
    "#     print(f'Example:\\n{input[0]}\\n{tokenizer.tokenize(input[0])}')\n",
    "    tokenized_data = tokenizer(text=input, max_length=input_n_tokens, truncation=True, padding=\"max_length\")\n",
    "    tokenized_data[\"labels\"] = tokenizer(text_target=target, max_length=target_n_tokens, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    return tokenized_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:28.662044Z",
     "iopub.status.busy": "2023-07-10T18:23:28.661629Z",
     "iopub.status.idle": "2023-07-10T18:23:29.490387Z",
     "shell.execute_reply": "2023-07-10T18:23:29.489387Z",
     "shell.execute_reply.started": "2023-07-10T18:23:28.662008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d48f782275e4384a2d2a27d628d5f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed756dd2dd1466eaeb76159cc22f60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e748b3e32b0243348b22355a4c0cf34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 3195, 16, 8, 1270, 10, 1079, 205, 109, 15, 7, 15, 11, 112, 2512, 13560, 26765, 3, 5, 37, 31777, 53, 583, 13, 7759, 65, 641, 5241, 1079, 205, 109, 15, 7, 15, 139, 5362, 3629, 6, 224, 38, 3, 14138, 112, 1518, 18, 413, 12373, 3, 31, 188, 40, 23, 21208, 3351, 31, 6, 11, 237, 1735, 12, 430, 684, 5, 299, 34, 133, 1727, 3, 88, 19, 341, 479, 21, 1155, 12, 453, 112, 14272, 3, 9, 12660, 5, 37, 5788, 63, 20737, 2213, 65, 17046, 15, 26, 30, 3, 9, 1048, 13, 814, 6377, 7, 11, 3814, 1302, 3, 88, 3, 22148, 383, 112, 1415, 5, 37, 1173, 560, 3, 9, 10851, 18, 15548, 18691, 261, 16, 8, 814, 5788, 63, 20737, 275, 37, 6679, 350, 12977, 6, 84, 19, 271, 1860, 44, 5610, 19446, 5, 290, 19, 92, 3, 9, 7434, 1202, 84, 1267, 8, 5788, 63, 20737, 4811, 692, 17056, 10681, 6, 10565, 44, 5610, 357, 21316, 6, 11, 3, 9, 3814, 1202, 13, 8, 2431, 1699, 210, 40, 17, 63, 10677, 7, 3112, 16, 84, 205, 109, 15, 7, 15, 38, 23711, 1699, 210, 40, 17, 63, 3, 189, 52, 23604, 112, 1131, 8513, 209, 5426, 28, 3, 9, 2195, 6421, 5, 37, 1302, 33, 859, 2391, 13, 205, 109, 15, 7, 15, 5505, 16, 20737, 11, 1699, 210, 40, 17, 63, 10677, 7, 27294, 84, 6, 28, 8, 119, 1173, 6, 33, 21, 1048, 30, 8, 8465, 23018, 52, 14237, 9, 1958, 475, 5, 94, 19, 8, 337, 475, 8, 3, 4552, 18, 1201, 18, 1490, 7556, 3150, 261, 12, 1789, 112, 11479, 12701, 29262, 18516, 6, 84, 47, 2944, 21, 5610, 2517, 6, 2915, 5, 282, 294, 13, 8, 1154, 3, 88, 4686, 12, 43, 3, 5, 3074, 28, 1], [21603, 10, 18263, 14373, 6585, 52, 21475, 9, 15, 40, 242, 7, 12019, 7865, 3, 9, 336, 18, 6890, 211, 107, 12, 761, 227, 3, 20508, 24, 3, 88, 141, 118, 1213, 57, 57, 3, 9, 1712, 3823, 416, 12, 112, 443, 5, 37, 5400, 18, 1201, 18, 1490, 584, 89, 434, 18372, 4884, 6585, 52, 6, 113, 92, 141, 10783, 7, 44, 12961, 12530, 6, 15922, 11, 23370, 6, 19, 22490, 12, 10003, 6, 11, 7760, 3, 88, 2654, 31, 17, 129, 139, 112, 443, 250, 3, 9, 1712, 47, 3823, 416, 12, 34, 5, 37, 22896, 3, 13106, 16, 8, 1455, 13, 112, 629, 6, 3355, 8, 3110, 630, 5888, 21, 460, 676, 5, 21475, 9, 15, 40, 242, 7, 12019, 5111, 3, 88, 47, 1213, 95, 21, 761, 250, 3, 9, 1712, 19653, 376, 45, 7084, 112, 443, 3, 5, 242, 7, 12019, 258, 27975, 3, 9, 1554, 13, 21168, 6, 2145, 10, 3, 31, 196, 31, 51, 22490, 12, 10003, 5, 27, 174, 204, 1175, 204, 761, 233, 7, 15, 51, 23, 18, 7, 75, 1208, 233, 346, 35, 132, 230, 314, 81, 460, 1109, 7, 3, 27315, 581, 82, 7859, 5, 31, 37, 28124, 6585, 52, 1869, 3, 9, 5112, 16, 1566, 3370, 344, 6260, 11, 2628, 28, 14373, 6, 12961, 12530, 11, 15922, 6, 10389, 147, 943, 5533, 1766, 5, 621, 3, 9, 386, 18, 1201, 10783, 16, 3434, 28, 23510, 16, 8, 28117, 6, 3, 88, 3666, 12, 2789, 28, 23370, 16, 8558, 68, 47, 2604, 924, 16, 627, 3179, 7, 5, 242, 7, 12019, 6, 113, 65, 2838, 1766, 16, 3, 3840, 1031, 21, 16458, 6, 65, 5799, 80, 1288, 16, 1296, 3179, 7, 48, 774, 5, 37, 5400, 18, 1201, 18, 1490, 28124, 6585, 52, 230, 4805, 21, 2968, 596, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[20737, 2213, 1079, 205, 109, 15, 7, 15, 6, 3, 4552, 6, 4464, 4509, 2512, 13560, 26765, 336, 215, 3, 5, 2043, 1967, 565, 26, 71, 120, 565, 1699, 63, 15, 6628, 8738, 16170, 16, 2628, 11, 5563, 12, 726, 160, 5610, 9368, 755, 51, 3, 5, 25263, 10851, 18, 15548, 18691, 45, 5788, 63, 20737, 11, 8, 6679, 350, 12977, 21, 5610, 19446, 3, 5, 1203, 3014, 1302, 13, 17056, 10681, 11, 1699, 210, 40, 17, 63, 10677, 7, 443, 3, 189, 12380, 53, 3112, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [21475, 9, 15, 40, 242, 7, 12019, 1083, 4805, 16, 21, 2968, 596, 18372, 4884, 16, 28117, 204, 3, 5, 37, 28124, 6585, 52, 47, 1213, 95, 21, 761, 57, 3, 9, 1712, 3823, 416, 12, 112, 443, 3, 5, 37, 5400, 18, 1201, 18, 1490, 19, 22490, 12, 10003, 6, 11, 80, 47, 3823, 57, 112, 443, 3, 5, 242, 7, 12019, 141, 10783, 7, 44, 14373, 6, 12961, 12530, 6, 15922, 11, 23370, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(input=x_train[:2], target=y_train[:2], input_n_tokens=INPUT_N_TOKENS, \n",
    "         target_n_tokens=TARGET_N_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:23:29.492550Z",
     "iopub.status.busy": "2023-07-10T18:23:29.491921Z",
     "iopub.status.idle": "2023-07-10T18:24:32.619614Z",
     "shell.execute_reply": "2023-07-10T18:24:32.618555Z",
     "shell.execute_reply.started": "2023-07-10T18:23:29.492512Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenize(input=x_train, target=y_train, input_n_tokens=INPUT_N_TOKENS, \n",
    "         target_n_tokens=TARGET_N_TOKENS)\n",
    "tokenized_test = tokenize(input=x_test, target=y_test, input_n_tokens=INPUT_N_TOKENS, \n",
    "         target_n_tokens=TARGET_N_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:24:32.621527Z",
     "iopub.status.busy": "2023-07-10T18:24:32.621144Z",
     "iopub.status.idle": "2023-07-10T18:24:32.629050Z",
     "shell.execute_reply": "2023-07-10T18:24:32.627810Z",
     "shell.execute_reply.started": "2023-07-10T18:24:32.621488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:24:32.631492Z",
     "iopub.status.busy": "2023-07-10T18:24:32.630790Z",
     "iopub.status.idle": "2023-07-10T18:24:32.642121Z",
     "shell.execute_reply": "2023-07-10T18:24:32.640847Z",
     "shell.execute_reply.started": "2023-07-10T18:24:32.631457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=300, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:24:32.644242Z",
     "iopub.status.busy": "2023-07-10T18:24:32.643867Z",
     "iopub.status.idle": "2023-07-10T18:24:32.661396Z",
     "shell.execute_reply": "2023-07-10T18:24:32.660350Z",
     "shell.execute_reply.started": "2023-07-10T18:24:32.644210Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_tf_tensors(data):\n",
    "    data = tf.data.Dataset.from_tensor_slices(dict(data))\n",
    "    data = data.prefetch(tf.data.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:24:32.664866Z",
     "iopub.status.busy": "2023-07-10T18:24:32.664563Z",
     "iopub.status.idle": "2023-07-10T18:25:18.599708Z",
     "shell.execute_reply": "2023-07-10T18:25:18.598648Z",
     "shell.execute_reply.started": "2023-07-10T18:24:32.664840Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tf_data = return_tf_tensors(tokenized_train)\n",
    "test_tf_data = return_tf_tensors(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:25:18.601906Z",
     "iopub.status.busy": "2023-07-10T18:25:18.601446Z",
     "iopub.status.idle": "2023-07-10T18:25:18.717420Z",
     "shell.execute_reply": "2023-07-10T18:25:18.716314Z",
     "shell.execute_reply.started": "2023-07-10T18:25:18.601803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(300,), dtype=int32, numpy=\n",
      "array([21603,    10,  3195,    16,     8,  1270,    10,  1079,   205,\n",
      "         109,    15,     7,    15,    11,   112,  2512, 13560, 26765,\n",
      "           3,     5,    37, 31777,    53,   583,    13,  7759,    65,\n",
      "         641,  5241,  1079,   205,   109,    15,     7,    15,   139,\n",
      "        5362,  3629,     6,   224,    38,     3, 14138,   112,  1518,\n",
      "          18,   413, 12373,     3,    31,   188,    40,    23, 21208,\n",
      "        3351,    31,     6,    11,   237,  1735,    12,   430,   684,\n",
      "           5,   299,    34,   133,  1727,     3,    88,    19,   341,\n",
      "         479,    21,  1155,    12,   453,   112, 14272,     3,     9,\n",
      "       12660,     5,    37,  5788,    63, 20737,  2213,    65, 17046,\n",
      "          15,    26,    30,     3,     9,  1048,    13,   814,  6377,\n",
      "           7,    11,  3814,  1302,     3,    88,     3, 22148,   383,\n",
      "         112,  1415,     5,    37,  1173,   560,     3,     9, 10851,\n",
      "          18, 15548, 18691,   261,    16,     8,   814,  5788,    63,\n",
      "       20737,   275,    37,  6679,   350, 12977,     6,    84,    19,\n",
      "         271,  1860,    44,  5610, 19446,     5,   290,    19,    92,\n",
      "           3,     9,  7434,  1202,    84,  1267,     8,  5788,    63,\n",
      "       20737,  4811,   692, 17056, 10681,     6, 10565,    44,  5610,\n",
      "         357, 21316,     6,    11,     3,     9,  3814,  1202,    13,\n",
      "           8,  2431,  1699,   210,    40,    17,    63, 10677,     7,\n",
      "        3112,    16,    84,   205,   109,    15,     7,    15,    38,\n",
      "       23711,  1699,   210,    40,    17,    63,     3,   189,    52,\n",
      "       23604,   112,  1131,  8513,   209,  5426,    28,     3,     9,\n",
      "        2195,  6421,     5,    37,  1302,    33,   859,  2391,    13,\n",
      "         205,   109,    15,     7,    15,  5505,    16, 20737,    11,\n",
      "        1699,   210,    40,    17,    63, 10677,     7, 27294,    84,\n",
      "           6,    28,     8,   119,  1173,     6,    33,    21,  1048,\n",
      "          30,     8,  8465, 23018,    52, 14237,     9,  1958,   475,\n",
      "           5,    94,    19,     8,   337,   475,     8,     3,  4552,\n",
      "          18,  1201,    18,  1490,  7556,  3150,   261,    12,  1789,\n",
      "         112, 11479, 12701, 29262, 18516,     6,    84,    47,  2944,\n",
      "          21,  5610,  2517,     6,  2915,     5,   282,   294,    13,\n",
      "           8,  1154,     3,    88,  4686,    12,    43,     3,     5,\n",
      "        3074,    28,     1], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(300,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>, 'labels': <tf.Tensor: shape=(150,), dtype=int32, numpy=\n",
      "array([20737,  2213,  1079,   205,   109,    15,     7,    15,     6,\n",
      "           3,  4552,     6,  4464,  4509,  2512, 13560, 26765,   336,\n",
      "         215,     3,     5,  2043,  1967,   565,    26,    71,   120,\n",
      "         565,  1699,    63,    15,  6628,  8738, 16170,    16,  2628,\n",
      "          11,  5563,    12,   726,   160,  5610,  9368,   755,    51,\n",
      "           3,     5, 25263, 10851,    18, 15548, 18691,    45,  5788,\n",
      "          63, 20737,    11,     8,  6679,   350, 12977,    21,  5610,\n",
      "       19446,     3,     5,  1203,  3014,  1302,    13, 17056, 10681,\n",
      "          11,  1699,   210,    40,    17,    63, 10677,     7,   443,\n",
      "           3,   189, 12380,    53,  3112,     3,     5,     1,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for i in train_tf_data.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:25:18.719959Z",
     "iopub.status.busy": "2023-07-10T18:25:18.718923Z",
     "iopub.status.idle": "2023-07-10T18:25:18.728418Z",
     "shell.execute_reply": "2023-07-10T18:25:18.727460Z",
     "shell.execute_reply.started": "2023-07-10T18:25:18.719920Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(train_data, val_data, epochs=2, eta=1e-4, early_stopping_patience=1, batch_size=BATCH_SIZE):\n",
    "    with strategy.scope():\n",
    "        model = TFAutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(eta))\n",
    "\n",
    "    print(model.summary())\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, mode=\"min\")\n",
    "    model.fit(train_data.shuffle(len(train_data)).batch(batch_size), validation_data=val_data.shuffle(len(val_data)).batch(batch_size), \n",
    "          epochs=epochs, callbacks=[early_stop])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:25:18.730529Z",
     "iopub.status.busy": "2023-07-10T18:25:18.730070Z",
     "iopub.status.idle": "2023-07-10T18:41:34.423986Z",
     "shell.execute_reply": "2023-07-10T18:41:34.422809Z",
     "shell.execute_reply.started": "2023-07-10T18:25:18.730491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2238438066494bd99360c79130fc6a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7df569fe6324f038699998862870768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "563/563 [==============================] - 523s 779ms/step - loss: 1.3561 - val_loss: 1.0234\n",
      "Epoch 2/2\n",
      "563/563 [==============================] - 433s 769ms/step - loss: 1.1316 - val_loss: 1.0146\n"
     ]
    }
   ],
   "source": [
    "model = fit_model(train_data=train_tf_data, val_data=test_tf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:41:37.841735Z",
     "iopub.status.busy": "2023-07-10T18:41:37.840576Z",
     "iopub.status.idle": "2023-07-10T18:41:37.847812Z",
     "shell.execute_reply": "2023-07-10T18:41:37.846852Z",
     "shell.execute_reply.started": "2023-07-10T18:41:37.841684Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference_tokenize(input: list, n_tokens: int):\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(CHECKPOINT)\n",
    "    tokenized_data = tokenizer(text=input, max_length=n_tokens, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "    return tokenizer, tokenized_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:41:57.152808Z",
     "iopub.status.busy": "2023-07-10T18:41:57.151412Z",
     "iopub.status.idle": "2023-07-10T18:41:57.161891Z",
     "shell.execute_reply": "2023-07-10T18:41:57.160633Z",
     "shell.execute_reply.started": "2023-07-10T18:41:57.152761Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(txt: str):\n",
    "    test_data = [\"summarize: \" + txt]\n",
    "    inference_tokenizer, tokenized_data = inference_tokenize(input=test_data, n_tokens=INPUT_N_TOKENS)\n",
    "    pred = model.generate(**tokenized_data, max_new_tokens=TARGET_N_TOKENS)\n",
    "    result = inference_tokenizer.decode(pred[0])\n",
    "    result = re.sub(\"<.*?>\", \"\", result).strip()\n",
    "    print(f\"INPUT TEXT:\\n{txt}\\n\\nSUMMARY:\\n{result}\")\n",
    "    return (txt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:41:58.365271Z",
     "iopub.status.busy": "2023-07-10T18:41:58.364867Z",
     "iopub.status.idle": "2023-07-10T18:42:10.614339Z",
     "shell.execute_reply": "2023-07-10T18:42:10.613295Z",
     "shell.execute_reply.started": "2023-07-10T18:41:58.365239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "Heavy rainfall in several parts of north India has plunged the region into chaos, with more than 28 reported dead in the past three days. Cities and towns are grappling with the aftermath as roads and buildings remain submerged in knee-deep water, including the capital Delhi where the situation is expected to worsen as the weather department predicts more downpour in the coming days.\n",
      "\n",
      "All schools in Delhi and Gurugram have been closed today due to the heavy rain-induced waterlogging. Videos show Delhi’s Connaught Place submerged in water prompting a Twitter user to brand it “Connaught River”.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "Heavy rainfall in several parts of north India has plunged the region into chaos. More than 28 people have died in the past three days. All schools in Delhi and Gurugram have been closed due to the heavy rain-induced waterlogging.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "Heavy rainfall in several parts of north India has plunged the region into chaos, with more than 28 reported dead in the past three days. Cities and towns are grappling with the aftermath as roads and buildings remain submerged in knee-deep water, including the capital Delhi where the situation is expected to worsen as the weather department predicts more downpour in the coming days.\n",
    "\n",
    "All schools in Delhi and Gurugram have been closed today due to the heavy rain-induced waterlogging. Videos show Delhi’s Connaught Place submerged in water prompting a Twitter user to brand it “Connaught River”.\n",
    "'''\n",
    "txt, result = inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:42:26.153491Z",
     "iopub.status.busy": "2023-07-10T18:42:26.152344Z",
     "iopub.status.idle": "2023-07-10T18:42:41.024816Z",
     "shell.execute_reply": "2023-07-10T18:42:41.023514Z",
     "shell.execute_reply.started": "2023-07-10T18:42:26.153427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to up to 15 years of experience by three months amidst project commencement delays, multiple sources in the know told Moneycontrol. This is coming at a time when the Indian IT sector has already taken a hit by macroeconomic headwinds and is facing project deferrals and ramp-downs as their clients cut down on tech budgets.\n",
      "\n",
      "Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore to name a few are impacted by the delays.\n",
      "\n",
      "These joiners were hired between January and April and were initially facing onboarding delays by a month. Many of them got two to three subsequent new joining dates. On July 10, however, many received emails stating that their joining dates are getting pushed to October.\n",
      "\n",
      "The information about a deferral in onboarding date hasn’t even been communicated proactively to those waiting to join the company, according to multiple people Moneycontrol spoke to.\n",
      "\n",
      "SUMMARY:\n",
      "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to 15 years of experience by three months. Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore are impacted by the delays. Many of them got two to three subsequent joining dates.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to up to 15 years of experience by three months amidst project commencement delays, multiple sources in the know told Moneycontrol. This is coming at a time when the Indian IT sector has already taken a hit by macroeconomic headwinds and is facing project deferrals and ramp-downs as their clients cut down on tech budgets.\n",
    "\n",
    "Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore to name a few are impacted by the delays.\n",
    "\n",
    "These joiners were hired between January and April and were initially facing onboarding delays by a month. Many of them got two to three subsequent new joining dates. On July 10, however, many received emails stating that their joining dates are getting pushed to October.\n",
    "\n",
    "The information about a deferral in onboarding date hasn’t even been communicated proactively to those waiting to join the company, according to multiple people Moneycontrol spoke to.'''\n",
    "txt, result = inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:42:50.484452Z",
     "iopub.status.busy": "2023-07-10T18:42:50.483992Z",
     "iopub.status.idle": "2023-07-10T18:43:00.707261Z",
     "shell.execute_reply": "2023-07-10T18:43:00.705696Z",
     "shell.execute_reply.started": "2023-07-10T18:42:50.484396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24, the Finance Ministry said on July 9.\n",
      "\n",
      "As per the ministry, as on July 9, the direct tax collected was 15.87 percent higher compared to the same period last financial year. This collection is 26.05 percent of the total budget estimates of direct taxes for FY 2023-24.\n",
      "\n",
      "Refunds amounting to Rs. 42,000 crore have been issued during April 1 to July 9, which are 2.55 percent higher than refunds issued during the same period in the preceding year, the ministry said.\n",
      "\n",
      "The overall rate of growth is faster than what the government had anticipated. In the Union Budget for 2023-24, presented by Finance Minister Nirmala Sitharaman on February 1, direct tax collections were seen rising 10.5 percent from 2022-23.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24. The direct tax collected was 15.87 percent higher compared to the same period last financial year. Refunds amounting to Rs. 42,000 crore have been issued between April 1 and July 9.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24, the Finance Ministry said on July 9.\n",
    "\n",
    "As per the ministry, as on July 9, the direct tax collected was 15.87 percent higher compared to the same period last financial year. This collection is 26.05 percent of the total budget estimates of direct taxes for FY 2023-24.\n",
    "\n",
    "Refunds amounting to Rs. 42,000 crore have been issued during April 1 to July 9, which are 2.55 percent higher than refunds issued during the same period in the preceding year, the ministry said.\n",
    "\n",
    "The overall rate of growth is faster than what the government had anticipated. In the Union Budget for 2023-24, presented by Finance Minister Nirmala Sitharaman on February 1, direct tax collections were seen rising 10.5 percent from 2022-23.\n",
    "'''\n",
    "txt, result = inference(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T18:43:21.689239Z",
     "iopub.status.busy": "2023-07-10T18:43:21.688808Z",
     "iopub.status.idle": "2023-07-10T18:43:22.637806Z",
     "shell.execute_reply": "2023-07-10T18:43:22.636526Z",
     "shell.execute_reply.started": "2023-07-10T18:43:21.689203Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"models/t5_news_summarizer.h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD SAVED MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:36:54.287706Z",
     "iopub.status.busy": "2023-07-10T19:36:54.286859Z",
     "iopub.status.idle": "2023-07-10T19:36:54.293579Z",
     "shell.execute_reply": "2023-07-10T19:36:54.292241Z",
     "shell.execute_reply.started": "2023-07-10T19:36:54.287664Z"
    }
   },
   "outputs": [],
   "source": [
    "CHECKPOINT = \"t5-small\"\n",
    "INPUT_N_TOKENS = 300\n",
    "TARGET_N_TOKENS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:36:55.826981Z",
     "iopub.status.busy": "2023-07-10T19:36:55.826509Z",
     "iopub.status.idle": "2023-07-10T19:36:55.833347Z",
     "shell.execute_reply": "2023-07-10T19:36:55.831423Z",
     "shell.execute_reply.started": "2023-07-10T19:36:55.826939Z"
    }
   },
   "outputs": [],
   "source": [
    "def prod_inference_tokenize(input: list, n_tokens: int):\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(CHECKPOINT)\n",
    "    tokenized_data = tokenizer(text=input, max_length=n_tokens, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "    return tokenizer, tokenized_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:37:13.581117Z",
     "iopub.status.busy": "2023-07-10T19:37:13.580628Z",
     "iopub.status.idle": "2023-07-10T19:37:20.349393Z",
     "shell.execute_reply": "2023-07-10T19:37:20.348126Z",
     "shell.execute_reply.started": "2023-07-10T19:37:13.581078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = TFAutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "# loaded_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\n",
    "loaded_model.load_weights(\"models/t5_news_summarizer.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:37:20.352465Z",
     "iopub.status.busy": "2023-07-10T19:37:20.352093Z",
     "iopub.status.idle": "2023-07-10T19:37:20.391195Z",
     "shell.execute_reply": "2023-07-10T19:37:20.390271Z",
     "shell.execute_reply.started": "2023-07-10T19:37:20.352434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T20:02:57.912199Z",
     "iopub.status.busy": "2023-07-10T20:02:57.911792Z",
     "iopub.status.idle": "2023-07-10T20:02:57.919054Z",
     "shell.execute_reply": "2023-07-10T20:02:57.917696Z",
     "shell.execute_reply.started": "2023-07-10T20:02:57.912167Z"
    }
   },
   "outputs": [],
   "source": [
    "def loaded_model_inference(txt: str):\n",
    "    test_data = [\"summarize: \" + txt]\n",
    "    inference_tokenizer, tokenized_data = prod_inference_tokenize(input=test_data, n_tokens=INPUT_N_TOKENS)\n",
    "    pred = loaded_model.generate(**tokenized_data, max_new_tokens=TARGET_N_TOKENS)\n",
    "#     print(pred[0])\n",
    "    result = inference_tokenizer.decode(pred[0])\n",
    "    result = re.sub(\"<.*?>\", \"\", result).strip()\n",
    "    print(f\"INPUT TEXT:\\n{txt}\\n\\nSUMMARY:\\n{result}\")\n",
    "    return (txt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T20:02:59.030033Z",
     "iopub.status.busy": "2023-07-10T20:02:59.029499Z",
     "iopub.status.idle": "2023-07-10T20:03:08.423260Z",
     "shell.execute_reply": "2023-07-10T20:03:08.422152Z",
     "shell.execute_reply.started": "2023-07-10T20:02:59.029967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "Heavy rainfall in several parts of north India has plunged the region into chaos, with more than 28 reported dead in the past three days. Cities and towns are grappling with the aftermath as roads and buildings remain submerged in knee-deep water, including the capital Delhi where the situation is expected to worsen as the weather department predicts more downpour in the coming days.\n",
      "\n",
      "All schools in Delhi and Gurugram have been closed today due to the heavy rain-induced waterlogging. Videos show Delhi’s Connaught Place submerged in water prompting a Twitter user to brand it “Connaught River”.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "Heavy rainfall in several parts of north India has plunged the region into chaos. More than 28 people have died in the past three days. All schools in Delhi and Gurugram have been closed due to the heavy rain-induced waterlogging.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "Heavy rainfall in several parts of north India has plunged the region into chaos, with more than 28 reported dead in the past three days. Cities and towns are grappling with the aftermath as roads and buildings remain submerged in knee-deep water, including the capital Delhi where the situation is expected to worsen as the weather department predicts more downpour in the coming days.\n",
    "\n",
    "All schools in Delhi and Gurugram have been closed today due to the heavy rain-induced waterlogging. Videos show Delhi’s Connaught Place submerged in water prompting a Twitter user to brand it “Connaught River”.\n",
    "'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:37:56.509073Z",
     "iopub.status.busy": "2023-07-10T19:37:56.508348Z",
     "iopub.status.idle": "2023-07-10T19:38:15.700346Z",
     "shell.execute_reply": "2023-07-10T19:38:15.699082Z",
     "shell.execute_reply.started": "2023-07-10T19:37:56.509032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to up to 15 years of experience by three months amidst project commencement delays, multiple sources in the know told Moneycontrol. This is coming at a time when the Indian IT sector has already taken a hit by macroeconomic headwinds and is facing project deferrals and ramp-downs as their clients cut down on tech budgets.\n",
      "\n",
      "Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore to name a few are impacted by the delays.\n",
      "\n",
      "These joiners were hired between January and April and were initially facing onboarding delays by a month. Many of them got two to three subsequent new joining dates. On July 10, however, many received emails stating that their joining dates are getting pushed to October.\n",
      "\n",
      "The information about a deferral in onboarding date hasn’t even been communicated proactively to those waiting to join the company, according to multiple people Moneycontrol spoke to.\n",
      "\n",
      "SUMMARY:\n",
      "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to 15 years of experience by three months. Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore are impacted by the delays. Many of them got two to three subsequent joining dates.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "Tata Consultancy Services (TCS) is holding up onboarding of lateral hires with 1.8 to up to 15 years of experience by three months amidst project commencement delays, multiple sources in the know told Moneycontrol. This is coming at a time when the Indian IT sector has already taken a hit by macroeconomic headwinds and is facing project deferrals and ramp-downs as their clients cut down on tech budgets.\n",
    "\n",
    "Over 200 lateral recruits across cities including Bangalore, Pune, Kochi, Bhubaneswar, Delhi NCR, and Indore to name a few are impacted by the delays.\n",
    "\n",
    "These joiners were hired between January and April and were initially facing onboarding delays by a month. Many of them got two to three subsequent new joining dates. On July 10, however, many received emails stating that their joining dates are getting pushed to October.\n",
    "\n",
    "The information about a deferral in onboarding date hasn’t even been communicated proactively to those waiting to join the company, according to multiple people Moneycontrol spoke to.'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:38:24.209065Z",
     "iopub.status.busy": "2023-07-10T19:38:24.208586Z",
     "iopub.status.idle": "2023-07-10T19:38:38.794290Z",
     "shell.execute_reply": "2023-07-10T19:38:38.793101Z",
     "shell.execute_reply.started": "2023-07-10T19:38:24.209027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24, the Finance Ministry said on July 9.\n",
      "\n",
      "As per the ministry, as on July 9, the direct tax collected was 15.87 percent higher compared to the same period last financial year. This collection is 26.05 percent of the total budget estimates of direct taxes for FY 2023-24.\n",
      "\n",
      "Refunds amounting to Rs. 42,000 crore have been issued during April 1 to July 9, which are 2.55 percent higher than refunds issued during the same period in the preceding year, the ministry said.\n",
      "\n",
      "The overall rate of growth is faster than what the government had anticipated. In the Union Budget for 2023-24, presented by Finance Minister Nirmala Sitharaman on February 1, direct tax collections were seen rising 10.5 percent from 2022-23.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24. The direct tax collected was 15.87 percent higher compared to the same period last financial year. Refunds amounting to Rs. 42,000 crore have been issued between April 1 and July 9.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "The Union government has collected Rs 4.75 lakh crore in direct taxes in the first quarter of 2023-24, the Finance Ministry said on July 9.\n",
    "\n",
    "As per the ministry, as on July 9, the direct tax collected was 15.87 percent higher compared to the same period last financial year. This collection is 26.05 percent of the total budget estimates of direct taxes for FY 2023-24.\n",
    "\n",
    "Refunds amounting to Rs. 42,000 crore have been issued during April 1 to July 9, which are 2.55 percent higher than refunds issued during the same period in the preceding year, the ministry said.\n",
    "\n",
    "The overall rate of growth is faster than what the government had anticipated. In the Union Budget for 2023-24, presented by Finance Minister Nirmala Sitharaman on February 1, direct tax collections were seen rising 10.5 percent from 2022-23.\n",
    "'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:38:43.507097Z",
     "iopub.status.busy": "2023-07-10T19:38:43.506330Z",
     "iopub.status.idle": "2023-07-10T19:38:55.397322Z",
     "shell.execute_reply": "2023-07-10T19:38:55.396039Z",
     "shell.execute_reply.started": "2023-07-10T19:38:43.507051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "India, the world's second-largest sugar producer and a major exporter in recent years, will likely have a smaller role in the sugar export market going forward as its government-led ethanol program continues to expand, a report said on Monday.\n",
      "\n",
      "According to the report Asia Biofuel Outlook, produced by research firm BMI, a unit of Fitch Solutions, India's pursuit of increased ethanol blending in gasoline, as a way to cut the oil products' import bill and reduce carbon emissions, will continue to support global sugar prices.\n",
      "\n",
      "BMI says that there is currently a fast development of additional capacity to produce ethanol in India, where the biofuel is made mainly from sugarcane.\n",
      "\n",
      "As more ethanol plants start production, more of the country's sugarcane crop will be used to make the fuel, limiting the amount of sugar that will be produced.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "India will likely have a smaller role in the sugar export market, a report says. The report is produced by research firm BMI. The research firm says there is currently a fast development of additional capacity to produce ethanol in India.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "India, the world's second-largest sugar producer and a major exporter in recent years, will likely have a smaller role in the sugar export market going forward as its government-led ethanol program continues to expand, a report said on Monday.\n",
    "\n",
    "According to the report Asia Biofuel Outlook, produced by research firm BMI, a unit of Fitch Solutions, India's pursuit of increased ethanol blending in gasoline, as a way to cut the oil products' import bill and reduce carbon emissions, will continue to support global sugar prices.\n",
    "\n",
    "BMI says that there is currently a fast development of additional capacity to produce ethanol in India, where the biofuel is made mainly from sugarcane.\n",
    "\n",
    "As more ethanol plants start production, more of the country's sugarcane crop will be used to make the fuel, limiting the amount of sugar that will be produced.\n",
    "'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T19:39:05.729336Z",
     "iopub.status.busy": "2023-07-10T19:39:05.728830Z",
     "iopub.status.idle": "2023-07-10T19:39:25.538778Z",
     "shell.execute_reply": "2023-07-10T19:39:25.537186Z",
     "shell.execute_reply.started": "2023-07-10T19:39:05.729227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "On July 4, police arrested a Pakistani woman who entered Indian illegally in May and was found living in Greater Noida with a man she met online via a gaming app, PUBG. The 27-year-old woman had searched YouTube for ways to gain entry into India. However, the duo were released and the woman has requested the government to grant her and her kids the citizenship of India as she is in love with Sachin Meena. Seema says that she has adopted Hinduism and alleged that her ex-husband beats her in Pakistan. Here's the full story of a love story which started on PUBG.\n",
      "\n",
      "SUMMARY:\n",
      "Pakistani woman entered Indian illegally in May and was found living in Greater Noida. The 27-year-old had searched YouTube for ways to gain entry into India. The duo were released and the woman has requested the government to grant her and her kids the citizenship of India. Seema says that she has adopted Hinduism and alleged that her ex-husband beat her in Pakistan.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "On July 4, police arrested a Pakistani woman who entered Indian illegally in May and was found living in Greater Noida with a man she met online via a gaming app, PUBG. The 27-year-old woman had searched YouTube for ways to gain entry into India. However, the duo were released and the woman has requested the government to grant her and her kids the citizenship of India as she is in love with Sachin Meena. Seema says that she has adopted Hinduism and alleged that her ex-husband beats her in Pakistan. Here's the full story of a love story which started on PUBG.'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-10T20:11:57.973674Z",
     "iopub.status.busy": "2023-07-10T20:11:57.971114Z",
     "iopub.status.idle": "2023-07-10T20:12:12.504415Z",
     "shell.execute_reply": "2023-07-10T20:12:12.503016Z",
     "shell.execute_reply.started": "2023-07-10T20:11:57.973592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "\n",
      "Google's Med-PaLM 2 is reportedly being tested in various hospitals since April of this year.\n",
      "\n",
      "The AI chatbot designed to answer medical questions is based on Google's PaLM 2, the Large Language Model (LLM) it announced at Google I/O in May. The model will also power Bard, its conversational and generative AI chatbot.\n",
      "As reported by The Wall Street Journal, which managed to get their hands on an internal mail, Med-PaLM 2 was trained on a set of expert medical demonstrations and has been designed to be used for Healthcare related problems.\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "Med-PaLM 2 is being tested in various hospitals since April of this year. The AI chatbot is based on Google's PaLM 2, the Large Language Model (LLM) it announced at Google I/O in May. The model will also power Bard, its conversational and generative AI chatbot.\n"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "Google's Med-PaLM 2 is reportedly being tested in various hospitals since April of this year.\n",
    "\n",
    "The AI chatbot designed to answer medical questions is based on Google's PaLM 2, the Large Language Model (LLM) it announced at Google I/O in May. The model will also power Bard, its conversational and generative AI chatbot.\n",
    "As reported by The Wall Street Journal, which managed to get their hands on an internal mail, Med-PaLM 2 was trained on a set of expert medical demonstrations and has been designed to be used for Healthcare related problems.\n",
    "'''\n",
    "txt, result = loaded_model_inference(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
