{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Get sentence embeddings ([CLS] token embedding of last hidden state) from RoBERTa model that is fine-tuned on arxiv data for classification. The embeddings are then compared with SentenceTransformer's embeddings","metadata":{"tags":[]}},{"cell_type":"code","source":"# !pip install sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport numpy as np\nfrom numpy.linalg import norm\nimport pandas as pd\nimport string\nimport tensorflow as tf\nfrom transformers import RobertaTokenizerFast, TFRobertaModel, RobertaConfig\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.layers import Input\nfrom sentence_transformers import SentenceTransformer","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:23:57.472316Z","iopub.execute_input":"2023-11-14T10:23:57.472791Z","iopub.status.idle":"2023-11-14T10:24:20.360015Z","shell.execute_reply.started":"2023-11-14T10:23:57.472746Z","shell.execute_reply":"2023-11-14T10:24:20.358715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class TextPreprocessor:\n    def __init__(self, remove_punct: bool = True, remove_digits: bool = True,\n                 remove_stop_words: bool = True,\n                 remove_short_words: bool = True, minlen: int = 1, maxlen: int = 1, top_p: float = None,\n                 bottom_p: float = None):\n        self.remove_punct = remove_punct\n        self.remove_digits = remove_digits\n        self.remove_stop_words = remove_stop_words\n        self.remove_short_words = remove_short_words\n        self.minlen = minlen\n        self.maxlen = maxlen\n        self.top_p = top_p\n        self.bottom_p = bottom_p\n        self.words_to_remove = []\n        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n                           'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n                           'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n                           'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n                           'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n                           'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or',\n                           'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n                           'into', 'through', 'during', 'before', 'after', 'to', 'from',\n                           'in', 'out', 'on', 'off', 'further', 'then', 'once',\n                           'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n                           'other', 'such', 'only', 'own', 'same', 'so', 'than',\n                           'too', 'can', 'will', 'just', 'should',\n                           'now']\n\n        \n\n    @staticmethod\n    def __remove_double_whitespaces(string: str):\n        return \" \".join(string.split())\n    \n\n    def __remove_punct(self, string_series: pd.Series):\n        \"\"\"\n       Removes punctuations from the input string.\n       :param string_series: pd.Series, input string series\n       :return: pd.Series, cleaned string series\n       \"\"\"\n        clean_string_series = string_series.copy()\n        puncts = [r'\\n', r'\\r', r'\\t']\n        puncts.extend(list(string.punctuation))\n        for i in puncts:\n            clean_string_series = clean_string_series.str.replace(pat=i, repl=\" \", regex=False).copy()\n        return clean_string_series.map(self.__remove_double_whitespaces)\n\n    def __remove_digits(self, string_series: pd.Series):\n        \"\"\"\n       Removes digits from the input string.\n       :param string_series: pd.Series, input string series\n       :return: pd.Series, cleaned string series\n       \"\"\"\n        clean_string_series = string_series.str.replace(pat=r'\\d', repl=\" \", regex=True).copy()\n        return clean_string_series.map(self.__remove_double_whitespaces)\n \n\n    def __remove_stop_words(self, string_series: pd.Series):\n        \"\"\"\n       Removes stop words from the input string.\n       :param string_series: pd.Series, input string series\n       :return: pd.Series, cleaned string series\n       \"\"\"\n        def str_remove_stop_words(string: str):\n            stops = self.stop_words\n            return \" \".join([token for token in string.split() if token not in stops])\n\n        return string_series.map(str_remove_stop_words)\n\n    \n\n    def preprocess(self, string_series: pd.Series, dataset: str = \"train\"):\n        \"\"\"\n        Entry point.\n        :param string_series: pd.Series, input string series\n        :param dataset: str, \"train\" for training set, \"tesrt\" for val/dev/test set.\n        :return: pd.Series, cleaned string series\n        \"\"\"\n        string_series = string_series.str.lower()\n        if self.remove_punct:\n            string_series = self.__remove_punct(string_series=string_series)\n        if self.remove_digits:\n            string_series = self.__remove_digits(string_series=string_series)\n        if self.remove_stop_words:\n            string_series = self.__remove_stop_words(string_series=string_series)\n        \n\n        string_series = string_series.str.strip()\n        string_series.replace(to_replace=\"\", value=\"this is an empty message\", inplace=True)\n\n        return string_series","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:24:20.361694Z","iopub.execute_input":"2023-11-14T10:24:20.362573Z","iopub.status.idle":"2023-11-14T10:24:20.386428Z","shell.execute_reply.started":"2023-11-14T10:24:20.362537Z","shell.execute_reply":"2023-11-14T10:24:20.385413Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Fine-tuning code: https://github.com/ksv-muralidhar/hugging_face_tf_fine_tuning/blob/main/roberta_text_classification.ipynb\ndef load_finetuned_model():\n    '''\n    Function to load fine-tuned RoBERTa model to classify arxiv papers.\n    '''\n    model = tf.keras.models.load_model('arxiv_classifier_hf_roberta.h5', \n                                       custom_objects={\"TFRobertaModel\": transformers.TFRobertaModel})\n    model_checkpoint = \"roberta-base\"\n    top = model.get_layer('tf.__operators__.getitem_9').output\n    model = Model(inputs=model.input, outputs=top)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:24:20.389556Z","iopub.execute_input":"2023-11-14T10:24:20.390478Z","iopub.status.idle":"2023-11-14T10:24:20.411118Z","shell.execute_reply.started":"2023-11-14T10:24:20.390446Z","shell.execute_reply":"2023-11-14T10:24:20.409864Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_sent_transformer_embeddings(sent_transformer, txt):\n    '''\n    Function to get sentence embeddings from SentenceTransformer using\n    'roberta-base-nli-mean-tokens' chaeckpoint\n    '''\n    txt = text_preprocessor.preprocess(pd.Series(txt))[0]\n    embedding = sent_transformer.encode(txt, show_progress_bar=False)\n    return embedding","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:24:20.412909Z","iopub.execute_input":"2023-11-14T10:24:20.413321Z","iopub.status.idle":"2023-11-14T10:24:20.424527Z","shell.execute_reply.started":"2023-11-14T10:24:20.413292Z","shell.execute_reply":"2023-11-14T10:24:20.423408Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_finetuned_model_embeddings(model, txt):\n    '''\n    Function to extract [CLS] token embedding from the last hidden state of\n    fine-tuned RoBERTa model\n    '''\n    model_checkpoint = \"roberta-base\"\n    tokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\n    txt = text_preprocessor.preprocess(pd.Series(txt))[0]\n    txt = tokenizer([txt], \n          max_length=200, padding=\"max_length\", truncation=True, return_tensors=\"tf\")\n    embedding = model.predict([txt['input_ids'], txt['attention_mask']], verbose=0)[0]\n    return embedding","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:24:20.425926Z","iopub.execute_input":"2023-11-14T10:24:20.426574Z","iopub.status.idle":"2023-11-14T10:24:20.437914Z","shell.execute_reply.started":"2023-11-14T10:24:20.426545Z","shell.execute_reply":"2023-11-14T10:24:20.436898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_similarity_score(emb1, emb2):\n    '''\n    Function to compute cosine-similarity score.\n    '''\n    cos_sim = np.dot(emb1, emb2) / (norm(emb1) * norm(emb2))\n    return cos_sim","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:24:20.439324Z","iopub.execute_input":"2023-11-14T10:24:20.439877Z","iopub.status.idle":"2023-11-14T10:24:20.455198Z","shell.execute_reply.started":"2023-11-14T10:24:20.439847Z","shell.execute_reply":"2023-11-14T10:24:20.453617Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"text_preprocessor = TextPreprocessor()\nsent_transformer = SentenceTransformer('roberta-base-nli-mean-tokens')\nfinetuned_model = load_finetuned_model()\n\ndef compare_models(txt1, txt2):\n    '''\n    Function to return cosine similarity scores of embeddings of fine-tuned model and \n    SentenceTransformer embeddings\n    '''\n    sent_emb1 = get_sent_transformer_embeddings(sent_transformer, txt1)\n    sent_emb2 = get_sent_transformer_embeddings(sent_transformer, txt2)\n    \n    finetuned_model_emb1 = get_finetuned_model_embeddings(finetuned_model, txt1)\n    finetuned_model_emb2 = get_finetuned_model_embeddings(finetuned_model, txt2)\n    \n    print(f'Similarity score of fine-tuned model: {get_similarity_score(finetuned_model_emb1, finetuned_model_emb2)}')\n    print(f'Similarity score of sentence transformer: {get_similarity_score(sent_emb1, sent_emb2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# computer science\ntxt1 = '''\nEnterprise applications of Large Language Models (LLMs) hold promise for question answering on\nenterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise\nquestions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks\ntailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance\nLLM-based question answering by providing business context is not well understood. This study aims\nto evaluate the accuracy of LLM-powered question answering systems in the context of enterprise\nquestions and SQL databases, while also exploring the role of knowledge graphs in improving\naccuracy.\n'''\n\n# computer science\ntxt2 = '''\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. \nBy comparison, token-free models that operate directly on raw text (bytes or characters) have\nmany benefits: they can process text in any\nlanguage out of the box, they are more robust\nto noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character\nsequences are longer than token sequences,\npast work on token-free models has often introduced new model architectures designed\nto amortize the cost of operating directly on\nraw text. In this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte\nsequences\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:32.425459Z","iopub.execute_input":"2023-11-14T10:25:32.425876Z","iopub.status.idle":"2023-11-14T10:25:34.251201Z","shell.execute_reply.started":"2023-11-14T10:25:32.425843Z","shell.execute_reply":"2023-11-14T10:25:34.249998Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.9938491582870483\nSimilarity score of sentence transformer: 0.5998247265815735\n","output_type":"stream"}]},{"cell_type":"code","source":"# math\ntxt1 = '''\nIt is known that many different types of finite random subgraph models undergo quantitatively similar phase transitions around their percolation thresholds, and the proofs of\nthese results rely on isoperimetric properties of the underlying host graph. Recently, the authors showed that such a phase transition occurs in a large class of regular high-dimensional\nproduct graphs, generalising a classic result for the hypercube.\nIn this paper we give new isoperimetric inequalities for such regular high-dimensional\nproduct graphs, which generalise the well-known isoperimetric inequality of Harper for the\nhypercube, and are asymptotically sharp for a wide range of set sizes. We then use these\nisoperimetric properties to investigate the structure of the giant component L1 in supercritical percolation on these product graphs, that is, when p =\n1+œµ\nd\n, where d is the degree of\nthe product graph and œµ > 0 is a small enough constant.\nWe show that typically L1 has edge-expansion ‚Ñ¶\n1\nd ln d\n\u0001\n. Furthermore, we show that L1\nlikely contains a linear-sized subgraph with vertex-expansion ‚Ñ¶\n1\nd ln d\n\u0001\n. These results are\nbest possible up to the logarithmic factor in d.\nUsing these likely expansion properties, we determine, up to small polylogarithmic factors\nin d, the likely diameter of L1 as well as the typical mixing time of a lazy random walk on\nL1. Furthermore, we show the likely existence of a cycle of length ‚Ñ¶\nn\nd ln d\n\u0001\n. These results\nnot only generalise, but also improve substantially upon the known bounds in the case of\nthe hypercube, where in particular the likely diameter and typical mixing time of L1 were\npreviously only known to be polynomial in \n'''\n\n# computer science\ntxt2 = '''\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. \nBy comparison, token-free models that operate directly on raw text (bytes or characters) have\nmany benefits: they can process text in any\nlanguage out of the box, they are more robust\nto noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character\nsequences are longer than token sequences,\npast work on token-free models has often introduced new model architectures designed\nto amortize the cost of operating directly on\nraw text. In this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte\nsequences\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:10.251662Z","iopub.execute_input":"2023-11-14T10:25:10.252054Z","iopub.status.idle":"2023-11-14T10:25:12.855045Z","shell.execute_reply.started":"2023-11-14T10:25:10.252021Z","shell.execute_reply":"2023-11-14T10:25:12.853987Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.3928041160106659\nSimilarity score of sentence transformer: 0.7885901927947998\n","output_type":"stream"}]},{"cell_type":"code","source":"# stats\ntxt1 = '''\nCausal inference necessarily relies upon untestable assumptions; hence,\nit is crucial to assess the robustness of obtained results to violations of identification\nassumptions. However, such sensitivity analysis is only occasionally undertaken in\npractice, as many existing methods only apply to relatively simple models and their\nresults are often difficult to interpret. We take a more flexible approach to sensitivity\nanalysis and view it as a constrained stochastic optimization problem. We focus\non linear models with an unmeasured confounder and a potential instrument. We\nshow how the R2\n-calculus ‚Äì a set of algebraic rules that relates different (partial) R2\n-\nvalues and correlations ‚Äì can be applied to identify the bias of the k-class estimators\nand construct sensitivity models flexibly. We further show that the heuristic ‚Äúplug-in‚Äù\nsensitivity interval may not have any confidence guarantees; instead, we propose a\nboostrap approach to construct sensitivity intervals which perform well in numerical\nsimulations. We illustrate the proposed methods with a real study on the causal effect\nof education on earnings and provide user-friendly visualization tools.\n\n'''\n\n# stats\ntxt2 = '''\nWhen estimating causal effects, it is important to assess external validity, i.e., determine\nhow useful a given study is to inform a practical question for a specific target population. One\nchallenge is that the covariate distribution in the population underlying a study may be different\nfrom that in the target population. If some covariates are effect modifiers, the average treatment\neffect (ATE) may not generalize to the target population. To tackle this problem, we propose new\nmethods to generalize or transport the ATE from a source population to a target population, in\nthe case where the source and target populations have different sets of covariates. When the ATE\nin the target population is identified, we propose new doubly robust estimators and establish their\nrates of convergence and limiting distributions. Under regularity conditions, the doubly robust\nestimators provably achieve the efficiency bound and are locally asymptotic minimax optimal.\nA sensitivity analysis is provided when the identification assumptions fail. Simulation studies\nshow the advantages of the proposed doubly robust estimator over simple plug-in estimators.\nImportantly, we also provide minimax lower bounds and higher-order estimators of the target\nfunctionals. The proposed methods are applied in transporting causal effects of dietary intake on\nadverse pregnancy outcomes from an observational study to the whole U.S. female population.\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:12.859325Z","iopub.execute_input":"2023-11-14T10:25:12.860166Z","iopub.status.idle":"2023-11-14T10:25:15.342328Z","shell.execute_reply.started":"2023-11-14T10:25:12.860123Z","shell.execute_reply":"2023-11-14T10:25:15.341272Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.8682332038879395\nSimilarity score of sentence transformer: 0.8693523406982422\n","output_type":"stream"}]},{"cell_type":"code","source":"# stats\ntxt1 = '''\nGiven the wealth inequality worldwide, there is an urgent need to identify the mode of\nwealth exchange through which it arises. To address the research gap regarding models that\ncombine equivalent exchange and redistribution, this study compares an equivalent market\nexchange with redistribution based on power centers and a nonequivalent exchange with mutual\naid using the Polanyi, Graeber, and Karatani modes of exchange. Two new exchange models based\non multi-agent interactions are reconstructed following an econophysics approach for evaluating\nthe Gini index (inequality) and total exchange (economic flow). Exchange simulations indicate that\nthe evaluation parameter of the total exchange divided by the Gini index can be expressed by the\nsame saturated curvilinear approximate equation using the wealth transfer rate and time period of\nredistribution and the surplus contribution rate of the wealthy and the saving rate. However,\nconsidering the coercion of taxes and its associated costs and independence based on the morality\nof mutual aid, a nonequivalent exchange without return obligation is preferred. This is oriented\ntoward Graeber's baseline communism and Karatani's mode of exchange D, with implications for\nalternatives to the capitalist economy.\n'''\n\n# computer science\ntxt2 = '''\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. \nBy comparison, token-free models that operate directly on raw text (bytes or characters) have\nmany benefits: they can process text in any\nlanguage out of the box, they are more robust\nto noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character\nsequences are longer than token sequences,\npast work on token-free models has often introduced new model architectures designed\nto amortize the cost of operating directly on\nraw text. In this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte\nsequences\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:15.345895Z","iopub.execute_input":"2023-11-14T10:25:15.346343Z","iopub.status.idle":"2023-11-14T10:25:17.748157Z","shell.execute_reply.started":"2023-11-14T10:25:15.346309Z","shell.execute_reply":"2023-11-14T10:25:17.746797Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.3753003180027008\nSimilarity score of sentence transformer: 0.6677366495132446\n","output_type":"stream"}]},{"cell_type":"code","source":"# stats\ntxt1 = '''\nGiven the wealth inequality worldwide, there is an urgent need to identify the mode of\nwealth exchange through which it arises. To address the research gap regarding models that\ncombine equivalent exchange and redistribution, this study compares an equivalent market\nexchange with redistribution based on power centers and a nonequivalent exchange with mutual\naid using the Polanyi, Graeber, and Karatani modes of exchange. Two new exchange models based\non multi-agent interactions are reconstructed following an econophysics approach for evaluating\nthe Gini index (inequality) and total exchange (economic flow). Exchange simulations indicate that\nthe evaluation parameter of the total exchange divided by the Gini index can be expressed by the\nsame saturated curvilinear approximate equation using the wealth transfer rate and time period of\nredistribution and the surplus contribution rate of the wealthy and the saving rate. However,\nconsidering the coercion of taxes and its associated costs and independence based on the morality\nof mutual aid, a nonequivalent exchange without return obligation is preferred. This is oriented\ntoward Graeber's baseline communism and Karatani's mode of exchange D, with implications for\nalternatives to the capitalist economy.\n'''\n\n# physics\ntxt2 = '''\nWe develop a neural network based pipeline to estimate masses of galaxy clusters with a \nknown redshift directly from photon information in X-rays. Our neural networks are trained \nusing supervised learning on simulations of eROSITA observations, focusing in this\npaper on the Final Equatorial Depth Survey (eFEDS). We use convolutional neural networks which are modified to include additional\ninformation of the cluster, in particular its redshift. In contrast to existing work, we utilize simulations including background and point\nsources to develop a tool which is usable directly on observational eROSITA data for an extended mass range from group size halos to\nmassive clusters with masses in between 1013M\f < M < 1015M\f. Using this method, we are able to provide for the first time neural\nnetwork mass estimation for the observed eFEDS cluster sample from Spectrum-Roentgen-Gamma/eROSITA observations and we\nfind consistent performance with weak lensing calibrated masses. In this measurement, we do not use weak lensing information and\nwe only use previous cluster mass information which was used to calibrate the cluster properties in the simulations. When compared\nto simulated data, we observe a reduced scatter with respect to luminosity and count-rate based scaling relations. We comment on the\napplication for other upcoming eROSITA All-Sky Survey observations.\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:17.751604Z","iopub.execute_input":"2023-11-14T10:25:17.751992Z","iopub.status.idle":"2023-11-14T10:25:19.984792Z","shell.execute_reply.started":"2023-11-14T10:25:17.751942Z","shell.execute_reply":"2023-11-14T10:25:19.983346Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.18251608312129974\nSimilarity score of sentence transformer: 0.8016979098320007\n","output_type":"stream"}]},{"cell_type":"code","source":"# physics\ntxt1 = '''\nThis search for Magnetic Monopoles (MMs) and High Electric Charge Objects (HECOs) with\nspins 0, 1/2 and 1, uses for the first time the full MoEDAL detector, exposed to 6.6 fb‚àí1 \nprotonproton collisions at 13 TeV. The results are interpreted in terms of Drell-Yan and photon-fusion\npair production. Mass limits on direct production of MMs of up to 10 Dirac magnetic charges and\nHECOs with electric charge in the range 5e to 350e, were achieved. The charge limits placed on\nMM and HECO production are currently the strongest in the world. MoEDAL is the only LHC\nexperiment capable of being directly calibrated for highly-ionizing particles using heavy ions and\nwith a detector system dedicated to definitively measuring magnetic charge.\n\n'''\n\n# physics\ntxt2 = '''\nRecently, a thermodynamic definition of time has been introduced. This definition is useful to find approach some open\nproblems in physics. But, it was obtained by a phenomenological approach and a logical inconsistency appears in the\ndefinition. In particular, the definition was based on the ratio of two quantities, the entropy production and its rate, linked one\nanother just by the definition of time. In this paper, this inconsistency is overcome, by using the second law of thermodynamics\nand Barbour‚Äôs mathematical methods, obtaining an analytical result that brings to the same equation of the phenomenological\nmethod, but without any logical inconsistency.\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:19.989838Z","iopub.execute_input":"2023-11-14T10:25:19.990839Z","iopub.status.idle":"2023-11-14T10:25:22.131164Z","shell.execute_reply.started":"2023-11-14T10:25:19.990788Z","shell.execute_reply":"2023-11-14T10:25:22.129787Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.7753373384475708\nSimilarity score of sentence transformer: 0.6930670142173767\n","output_type":"stream"}]},{"cell_type":"code","source":"# physics\ntxt1 = '''\nThis search for Magnetic Monopoles (MMs) and High Electric Charge Objects (HECOs) with\nspins 0, 1/2 and 1, uses for the first time the full MoEDAL detector, exposed to 6.6 fb‚àí1 \nprotonproton collisions at 13 TeV. The results are interpreted in terms of Drell-Yan and photon-fusion\npair production. Mass limits on direct production of MMs of up to 10 Dirac magnetic charges and\nHECOs with electric charge in the range 5e to 350e, were achieved. The charge limits placed on\nMM and HECO production are currently the strongest in the world. MoEDAL is the only LHC\nexperiment capable of being directly calibrated for highly-ionizing particles using heavy ions and\nwith a detector system dedicated to definitively measuring magnetic charge.\n\n'''\n\n# economics\ntxt2 = '''\nIn this paper, we propose Forest-PLS, a feature selection method for analyzing policy \neffect heterogeneity in a more flexible and comprehensive manner than is typically\navailable with conventional methods. In particular, our method is able to capture policy \neffect heterogeneity both within and across subgroups of the population defined\nby observable characteristics. To achieve this, we employ partial least squares to identify \ntarget components of the population and causal forests to estimate personalized\npolicy effects across these components. We show that the method is consistent and\nleads to asymptotically normally distributed policy effects. To demonstrate the efficacy \nof our approach, we apply it to the data from the Pennsylvania Reemployment\nBonus Experiments, which were conducted in 1988-1989. The analysis reveals that financial \nincentives can motivate some young non-white individuals to enter the labor\nmarket. However, these incentives may also provide a temporary financial cushion\nfor others, dissuading them from actively seeking employment. Our findings highlight the \nneed for targeted, personalized measures for young non-white male participants.\n\n'''\n\ncompare_models(txt1, txt2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-14T10:25:22.135093Z","iopub.execute_input":"2023-11-14T10:25:22.135517Z","iopub.status.idle":"2023-11-14T10:25:24.112559Z","shell.execute_reply.started":"2023-11-14T10:25:22.135484Z","shell.execute_reply":"2023-11-14T10:25:24.111063Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.3002638518810272\nSimilarity score of sentence transformer: 0.6593513488769531\n","output_type":"stream"}]},{"cell_type":"code","source":"# economics\ntxt1 = '''\nGiven an initial matching and a policy objective on the distribution of agent types to institutions, we study the existence of a mechanism\nthat weakly improves the distributional objective and satisfies constrained\nefficiency, individual rationality, and strategy-proofness. We show that\nsuch a mechanism need not exist in general. We introduce a new notion\nof discrete concavity, which we call pseudo M‚ôÆ\n-concavity, and construct a\nmechanism with the desirable properties when the distributional objective\nsatisfies this notion. We provide several practically relevant distributional\nobjectives that are pseudo M‚ôÆ\n-concave.\n'''\n\n# economics\ntxt2 = '''\nAn agent may strategically employ a vague message to mislead an audience‚Äôs belief\nabout the state of the world, but this may cause the agent to feel guilt or negatively impact\nhow the audience perceives the agent. Using a novel experimental design that allows\nparticipants to be vague while at the same time isolating the internal cost of lying from\nthe social identity cost of appearing dishonest, we explore the extent to which these two\ntypes of lying costs affect communication. We find that participants exploit vagueness to be\nconsistent with the truth, while at the same time leveraging the imprecision to their own\nbenefit. More participants use vague messages in treatments where concern with social\nidentity is relevant. In addition, we find that social identity concerns substantially affect\nthe length and patterns of vague messages used across the treatments.\n\n'''\n\ncompare_models(txt1, txt2)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:25:24.116937Z","iopub.execute_input":"2023-11-14T10:25:24.117482Z","iopub.status.idle":"2023-11-14T10:25:26.154329Z","shell.execute_reply.started":"2023-11-14T10:25:24.117437Z","shell.execute_reply":"2023-11-14T10:25:26.153084Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.7802478075027466\nSimilarity score of sentence transformer: 0.6077088713645935\n","output_type":"stream"}]},{"cell_type":"code","source":"# economics\ntxt1 = '''\nThis paper examines the dynamics of Tether, the stablecoin with the largest\nmarket capitalization. We show that the distributional and dynamic properties of Tether/USD rates have been evolving from 2017 to 2021. We use local\nanalysis methods to detect and describe the local patterns, such as short-lived\ntrends, time-varying volatility and persistence. To accommodate these patterns, we consider a time varying parameter Double Autoregressive tvDAR(1)\nmodel under the assumption of local stationarity of Tether/USD rates. We estimate the tvDAR model non-parametrically and test hypotheses on the functional parameters. In the application to Tether, the model provides a good fit\nand reliable out-of-sample forecasts at short horizons, while being robust to\ntime-varying persistence and volatility. In addition, the model yields a simple\nplug-in measure of stability for Tether and other stablecoins for assessing and\ncomparing their stability.\n\n'''\n\n# electrical engg.\ntxt2 = '''\nIn this paper, we optimize a Wireless Powered Communication (WPC) system including multiple\npair of users, where transmitters employ single-antenna to transmit their information and power to their\nreceivers with the help of one multiple-antennas Amplify-and-Forward (AF) relay or an active Intelligent\nReflecting Surface (IRS). We propose a joint Time Switching (TS) scheme in which transmitters,\nreceivers, and the relay/IRS are either in their energy or information transmission/reception modes.\nThe transmitted multi-carrier unmodulated and modulated waveforms are used for Energy Harvesting\n(EH) and Information Decoding (ID) modes, respectively. In order to design an optimal fair system, we\nmaximize the minimum rate of all pairs for both relay and IRS systems through a unified framework.\nThis framework allows us to simultaneously design energy waveforms, find optimal relay/IRS amplification\n/reflection matrices, allocate powers for information waveforms, and allocate time durations for\nvarious phases. In addition, we take into account the non-linearity of the EH circuits in our problem. This\nproblem turns out to be non-convex. Thus, we propose an iterative algorithm by using the MinorizationMaximization (MM) \ntechnique, which quickly converges to the optimal solution. Numerical examples\nshow that the proposed method improves the performance of the multi-pair WPC relay/IRS system\nunder various setups.\n'''\n\ncompare_models(txt1, txt2)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:25:26.158635Z","iopub.execute_input":"2023-11-14T10:25:26.159485Z","iopub.status.idle":"2023-11-14T10:25:28.361681Z","shell.execute_reply.started":"2023-11-14T10:25:26.159427Z","shell.execute_reply":"2023-11-14T10:25:28.360403Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.4304705262184143\nSimilarity score of sentence transformer: 0.716398298740387\n","output_type":"stream"}]},{"cell_type":"code","source":"# math\ntxt1 = '''\nLet either GL(E) √ó SO(F) or GL(E) √ó Sp(F) act naturally on the space of\nmatrices E ‚äó F. There are only finitely many orbits, and the orbit closures are orthogonal\nand symplectic generalizations of determinantal varieties, which can be described similarly\nusing rank conditions. In this paper, we study the singularities of these varieties and\ndescribe their defining equations. We prove that in the symplectic case, the orbit closures\nare normal with good filtrations, and in characteristic 0 have rational singularities. In\nthe orthogonal case we show that most orbit closures will have the same properties, and\ndetermine precisely the exceptions to this.\n'''\n\n# math\ntxt2 = '''\n In this paper, we initiate the study of a triple (X, ‚àÜ, D) which consists of\na pair (X, ‚àÜ) and a polarizing pseudoeffective divisor D. The adjoint asymptotic \nmultiplier ideal sheaf J (X, ‚àÜ; kDk) associated to the triple gives a simultaneous \ngeneralization of the multiplier ideal sheaf J (D) and asymptotic multiplier ideal sheaf J (kDk).\nWe describe the closed set defined by the ideal sheaf J (X, ‚àÜ; kDk) in terms of the \nminimal model program. We also characterize the case where J (X, ‚àÜ; kDk) = OX. Lastly,\nwe also prove a Nadel type vanishing theorem of cohomology using J (X, ‚àÜ; kDk).\n'''\n\ncompare_models(txt1, txt2)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:25:28.364778Z","iopub.execute_input":"2023-11-14T10:25:28.365194Z","iopub.status.idle":"2023-11-14T10:25:30.347609Z","shell.execute_reply.started":"2023-11-14T10:25:28.365162Z","shell.execute_reply":"2023-11-14T10:25:30.346748Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.9861209392547607\nSimilarity score of sentence transformer: 0.720016598701477\n","output_type":"stream"}]},{"cell_type":"code","source":"# computer science\ntxt1 = '''\nInfluence Maximization (IM) is a crucial problem in data science.\nThe goal is to find a fixed-size set of highly-influential seed vertices\non a network to maximize the influence spread along the edges.\nWhile IM is NP-hard on commonly-used diffusion models, a greedy\nalgorithm can achieve (1‚àí1/ùëí)-approximation, repeatedly selecting\nthe vertex with the highest marginal gain in influence as the seed.\nDue to theoretical guarantees, rich literature focuses on improving\nthe performance of the greedy algorithm. To estimate the marginal\ngain, existing work either runs Monte Carlo (MC) simulations of\ninfluence spread or pre-stores hundreds of sketches (usually pervertex information). However, these approaches can be inefficient\nin time (MC simulation) or space (storing sketches), preventing the\nideas from scaling to today‚Äôs large-scale graphs.\nThis paper significantly improves the scalability of IM using\ntwo key techniques. The first is a sketch-compression technique for\nthe independent cascading model on undirected graphs. It allows\ncombining the simulation and sketching approaches to achieve\na time-space tradeoff. The second technique includes new data\nstructures for parallel seed selection. Using our new approaches,\nwe implemented PaC-IM: Parallel and Compressed IM.\nWe compare PaC-IM with state-of-the-art parallel IM systems\non a 96-core machine with 1.5TB memory. PaC-IM can process\nlarge-scale graphs with up to 900M vertices and 74B edges in about\n2 hours. On average across all tested graphs, our uncompressed\nversion is 5‚Äì18√ó faster and about 1.4√ó more space-efficient than\nexisting parallel IM systems. Using compression further saves 3.8√ó\nspace with only 70% overhead in time on average.\n\n'''\n\n# computer science\ntxt2 = '''\nThe planted clique problem is a paradigmatic model of statistical-to-computational\ngaps: the planted clique is information-theoretically detectable if its size k ‚â• 2 log2 n but \npolynomialtime algorithms only exist for the recovery task when k = ‚Ñ¶(‚àö\nn). By now, there are many simple\nand fast algorithms that succeed as soon as k = ‚Ñ¶(‚àö\nn). Glaringly, however, no MCMC approach\nto the problem had been shown to work, including the Metropolis process on cliques studied by\nJerrum since 1992. In fact, Chen, Mossel, and Zadik recently showed that any Metropolis process\nwhose state space is the set of cliques fails to find any sub-linear sized planted clique in polynomial\ntime if initialized naturally from the empty set. Here, we redeem MCMC performance for the\nplanted clique problem by relaxing the state space to all vertex subsets and adding a corresponding\nenergy penalty for missing edges. With that, we prove that energy-minimizing Markov chains \n(gradient descent and a low-temperature relaxation of it) succeed at recovering planted cliques of size\nk = ‚Ñ¶(‚àö\nn) if initialized from the full graph. Importantly, initialized from the empty set, the \nrelaxation still does not help the gradient descent find sub-linear planted cliques. We also demonstrate\nrobustness of these Markov chain approaches under a natural contamination model.\n'''\n\ncompare_models(txt1, txt2)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T10:25:30.350459Z","iopub.execute_input":"2023-11-14T10:25:30.351016Z","iopub.status.idle":"2023-11-14T10:25:32.419758Z","shell.execute_reply.started":"2023-11-14T10:25:30.350986Z","shell.execute_reply":"2023-11-14T10:25:32.418428Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Similarity score of fine-tuned model: 0.9069778919219971\nSimilarity score of sentence transformer: 0.7196155786514282\n","output_type":"stream"}]}]}