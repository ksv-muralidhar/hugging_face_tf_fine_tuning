{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport re\nfrom tqdm import tqdm\nimport cloudpickle\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast, TFAutoModelForTokenClassification\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LabelEncoder:\n    '''\n    Label Encoder to encode and decode the entity labels\n    '''\n    def __init__(self):\n        self.label_mapping = {'O': 0, \n                             'B-geo': 1, \n                             'I-geo': 2, \n                             'B-gpe': 3, \n                             'I-gpe': 4, \n                             'B-per': 5,\n                             'I-per': 6,\n                             'B-org': 7,\n                             'I-org': 8,\n                             'B-tim': 9,\n                             'I-tim': 10,\n                             'B-art': 11, \n                             'I-art': 12,\n                             'B-nat': 13,\n                             'I-nat': 14,\n                             'B-eve': 15,\n                             'I-eve': 16,\n                             '[CLS]': -100,\n                             '[SEP]': -100}\n        \n        self.inverse_label_mapping = {}\n    \n    def fit(self, x: pd.Series):\n        self.inverse_label_mapping = {value: key for key, value in self.label_mapping.items()}\n        return self\n        \n    def transform(self, x: pd.Series):\n        x = x.map(self.label_mapping)\n        return x\n    \n    def inverse_transform(self, x: pd.Series):\n        x = x.map(self.inverse_label_mapping)\n        return x\n        ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:13.825703Z","iopub.execute_input":"2023-11-11T13:49:13.826435Z","iopub.status.idle":"2023-11-11T13:49:13.837531Z","shell.execute_reply.started":"2023-11-11T13:49:13.826403Z","shell.execute_reply":"2023-11-11T13:49:13.836561Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Fitting and saving Label Encoder\nlabel_encoder = LabelEncoder()\ndf = pd.read_csv('ner_dataset.csv', encoding='unicode_escape')\nlabel_encoder.fit(df['Tag'])\nwith open('hf_ner_label_encoder.bin', 'wb') as f:\n    cloudpickle.dump(label_encoder, f)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:13.838992Z","iopub.execute_input":"2023-11-11T13:49:13.839418Z","iopub.status.idle":"2023-11-11T13:49:14.808447Z","shell.execute_reply.started":"2023-11-11T13:49:13.839388Z","shell.execute_reply":"2023-11-11T13:49:14.807656Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# data source: https://www.kaggle.com/datasets/saurabhprajapat/named-entity-recognition\ndef get_preprocessed_data(file_path):\n    '''\n    Function to read the data from CSV and collect tokens and tags of each\n    sentence as lists.\n    '''\n    df = pd.read_csv(file_path, encoding='unicode_escape')\n    df = df.groupby('Sentence #', as_index=False).agg({'Tag': lambda x: list(x), 'Word': lambda x: list(x)})\n    df.drop(columns='Sentence #', inplace=True)\n    df.columns = ['target', 'text']\n    return df","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:14.810579Z","iopub.execute_input":"2023-11-11T13:49:14.810898Z","iopub.status.idle":"2023-11-11T13:49:14.818432Z","shell.execute_reply.started":"2023-11-11T13:49:14.810871Z","shell.execute_reply":"2023-11-11T13:49:14.817688Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_inputs_adjusted_labels(list_of_texts, list_of_labels, label_encoder, max_token_length=50):\n    '''\n    Function to rearrange the entity labels to match with the sub-word tokens and \n    [CLS], [PAD] and [SEP] tokens\n    '''\n    model_checkpoint = \"distilbert-base-uncased\"\n    tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n    \n    adjusted_labels = []\n    adjusted_encoded_labels = []\n    tokenized_inputs = {}\n    \n    for idx in range(len(list_of_texts)):\n        text = list_of_texts[idx]\n        labels = list_of_labels[idx]\n        \n        #####################################\n        #     Input Tokenization Start      #\n        #####################################\n    \n        inputs = tokenizer(text, is_split_into_words=True, max_length=max_token_length, truncation=True, padding=\"max_length\")\n        word_ids = inputs.word_ids()\n        # print(word_ids)\n        \n        \n        if len(tokenized_inputs) == 0:\n            tokenized_inputs['input_ids'] = [inputs['input_ids']]\n            tokenized_inputs['attention_mask'] = [inputs['attention_mask']]\n        else:\n            tokenized_inputs['input_ids'].append(inputs['input_ids'])\n            tokenized_inputs['attention_mask'].append(inputs['attention_mask'])\n        \n        #####################################\n        #     Input Tokenization End        #\n        #####################################\n        \n        #####################################\n        #     Label Rearrangement Start     #\n        #####################################\n        res = ['[CLS]']\n        p = 1\n        \n        while p < len(word_ids):\n            if word_ids[p] is None:\n                res.append('[SEP]')\n                p += 1\n                continue\n            prev_label = res[p-1]\n            curr_label = labels[word_ids[p]]\n            if prev_label.find('-') != -1:\n                prev_label_split = prev_label.split(\"-\")\n            else:\n                prev_label_split = ['PO', 'PO']\n            prev_label_prefix, prev_label_suffix = prev_label_split\n\n\n            if curr_label.find('-') != -1:\n                curr_label_split = curr_label.split(\"-\")\n            else:\n                curr_label_split = ['CO', 'CO']\n            curr_label_prefix, curr_label_suffix = curr_label_split\n\n            if ((prev_label_prefix == 'B') or (prev_label_prefix == 'I')) and (prev_label_suffix == curr_label_suffix):\n                res.append('I-'+prev_label_suffix)\n            else:\n                res.append(curr_label)\n            p += 1\n            \n        #####################################\n        #     Label Rearrangement End       #\n        #####################################\n        \n        adjusted_labels.append(res)\n        adjusted_encoded_labels.append([*label_encoder.transform(pd.Series(res))])\n    return tokenized_inputs, adjusted_labels, adjusted_encoded_labels","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:14.819553Z","iopub.execute_input":"2023-11-11T13:49:14.819835Z","iopub.status.idle":"2023-11-11T13:49:14.832532Z","shell.execute_reply.started":"2023-11-11T13:49:14.819813Z","shell.execute_reply":"2023-11-11T13:49:14.831778Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_processed_train_test(file_path='ner_dataset.csv', label_encoder_path='hf_ner_label_encoder.bin',\n                             test_size: float=0.15, input_col: str='text', target_col: str='target', \n                             max_token_length=50, random_state=42):\n    \n    '''\n    Function to read CSV data and return preprocessed train and test sets\n    '''\n    df = get_preprocessed_data(file_path)\n    x = df[input_col].copy()\n    y = df[target_col].copy()\n    del(df)\n    \n    if test_size > 0:\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n    else:\n        x_train, y_train = x, y\n        x_test, y_test = None, None\n    \n    x_train, y_train  = x_train.to_list(), y_train.to_list()\n    if x_test is not None:\n        x_test, y_test = x_test.to_list(), y_test.to_list()\n    \n    with open(label_encoder_path, 'rb') as f:\n        label_encoder = cloudpickle.load(f)\n    \n    x_train, _, y_train = get_inputs_adjusted_labels(x_train, y_train, label_encoder, max_token_length)\n    \n    train = x_train\n    train['labels'] = y_train\n    \n    if x_test is not None:\n        x_test, _, y_test = get_inputs_adjusted_labels(x_test, y_test, label_encoder, max_token_length)\n        test = x_test\n        test['labels'] = y_test\n    else:\n        test = None\n        \n    return train, test","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:14.833461Z","iopub.execute_input":"2023-11-11T13:49:14.833734Z","iopub.status.idle":"2023-11-11T13:49:14.847543Z","shell.execute_reply.started":"2023-11-11T13:49:14.833712Z","shell.execute_reply":"2023-11-11T13:49:14.846701Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def visualize_tokens_labels(idx):\n    '''\n    Function to visualize tokens and entity labels\n    before and after preprocessing\n    '''\n    df = get_preprocessed_data('ner_dataset.csv')\n    tokens = df.iloc[idx, 1]\n    labels = df.iloc[idx, 0]\n    print('BEFORE PREPROCESSING')\n    for tok, lab in zip(tokens, labels):\n        print(f'{tok: <20}{lab}')\n    \n    print('\\nAFTER PREPROCESSING')\n    x, _ = get_processed_train_test(max_token_length=50, test_size=0.0)\n    input_ids = x['input_ids'][idx]\n    labels = x['labels'][idx]\n    model_checkpoint = \"distilbert-base-uncased\"\n    tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n    tokens = tokenizer.convert_ids_to_tokens(x['input_ids'][idx])\n\n    label_encoder_path='hf_ner_label_encoder.bin'\n    with open(label_encoder_path, 'rb') as f:\n        label_encoder = cloudpickle.load(f)\n        \n    labels = [*label_encoder.inverse_transform(pd.Series(labels))]\n    for tok, lab in zip(tokens, labels):\n        print(f'{tok: <20}{lab}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:14.848743Z","iopub.execute_input":"2023-11-11T13:49:14.849273Z","iopub.status.idle":"2023-11-11T13:49:14.860602Z","shell.execute_reply.started":"2023-11-11T13:49:14.849213Z","shell.execute_reply":"2023-11-11T13:49:14.859787Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"visualize_tokens_labels(123)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:49:14.861685Z","iopub.execute_input":"2023-11-11T13:49:14.861951Z","iopub.status.idle":"2023-11-11T13:50:08.217299Z","shell.execute_reply.started":"2023-11-11T13:49:14.861929Z","shell.execute_reply":"2023-11-11T13:50:08.216366Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"BEFORE PREPROCESSING\nSpain               B-gpe\nhas                 O\nbegun               O\na                   O\ntrial               O\nfor                 O\n24                  O\nsuspected           O\nal-Qaida            B-org\nmembers             O\n,                   O\nincluding           O\nthree               O\naccused             O\nof                  O\nhelping             O\nplan                O\nthe                 O\nSeptember           B-tim\n11                  I-tim\n,                   I-tim\n2001                I-tim\nterrorist           O\nattacks             O\nin                  O\nthe                 O\nUnited              B-geo\nStates              I-geo\n.                   O\n\nAFTER PREPROCESSING\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1ce77ea841486d994968a883007f42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3217233b69db416ebdd25ef016e87432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5e8a33ff1d44048f3c4f2433cf3ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce3aab0b87545768f73e70c8ec2ebee"}},"metadata":{}},{"name":"stdout","text":"[CLS]               [SEP]\nspain               B-gpe\nhas                 O\nbegun               O\na                   O\ntrial               O\nfor                 O\n24                  O\nsuspected           O\nal                  B-org\n-                   I-org\nq                   I-org\n##aid               I-org\n##a                 I-org\nmembers             O\n,                   O\nincluding           O\nthree               O\naccused             O\nof                  O\nhelping             O\nplan                O\nthe                 O\nseptember           B-tim\n11                  I-tim\n,                   I-tim\n2001                I-tim\nterrorist           O\nattacks             O\nin                  O\nthe                 O\nunited              B-geo\nstates              I-geo\n.                   O\n[SEP]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n[PAD]               [SEP]\n","output_type":"stream"}]},{"cell_type":"code","source":"def return_tf_tensors(data):\n    data = tf.data.Dataset.from_tensor_slices(dict(data))\n    data = data.prefetch(tf.data.AUTOTUNE)\n    return data","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:50:08.218557Z","iopub.execute_input":"2023-11-11T13:50:08.218930Z","iopub.status.idle":"2023-11-11T13:50:08.224279Z","shell.execute_reply.started":"2023-11-11T13:50:08.218896Z","shell.execute_reply":"2023-11-11T13:50:08.223241Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"CHECKPOINT = \"distilbert-base-uncased\"\nN_TOKENS = 50\nBATCH_SIZE = 32\nN_LABELS = 18","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:50:08.227111Z","iopub.execute_input":"2023-11-11T13:50:08.227385Z","iopub.status.idle":"2023-11-11T13:50:08.235646Z","shell.execute_reply.started":"2023-11-11T13:50:08.227362Z","shell.execute_reply":"2023-11-11T13:50:08.234811Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train, test = get_processed_train_test(file_path='ner_dataset.csv', max_token_length=N_TOKENS, test_size=0.15)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:50:08.236713Z","iopub.execute_input":"2023-11-11T13:50:08.237032Z","iopub.status.idle":"2023-11-11T13:50:56.448446Z","shell.execute_reply.started":"2023-11-11T13:50:08.237001Z","shell.execute_reply":"2023-11-11T13:50:56.447477Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_tf_data = return_tf_tensors(train)\ntest_tf_data = return_tf_tensors(test)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:50:56.449703Z","iopub.execute_input":"2023-11-11T13:50:56.449973Z","iopub.status.idle":"2023-11-11T13:51:22.831157Z","shell.execute_reply.started":"2023-11-11T13:50:56.449950Z","shell.execute_reply":"2023-11-11T13:51:22.830308Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for i in train_tf_data.take(1):\n    print(i)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:51:22.832327Z","iopub.execute_input":"2023-11-11T13:51:22.832984Z","iopub.status.idle":"2023-11-11T13:51:22.891861Z","shell.execute_reply.started":"2023-11-11T13:51:22.832953Z","shell.execute_reply":"2023-11-11T13:51:22.891016Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'input_ids': <tf.Tensor: shape=(50,), dtype=int32, numpy=\narray([  101,  1996,  3732,  2181,  2001,  4015,  2000,  2660,  1999,\n        5774,  1010,  2029,  4548,  1996,  2642,  4664,  2076,  2088,\n        2162,  1045,  1998,  2506,  2000, 21497,  1996,  4117,  2752,\n        2127,  4336,  1999,  3339,  1012,   102,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0,     0,\n           0,     0,     0,     0,     0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(50,), dtype=int32, numpy=\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0], dtype=int32)>, 'labels': <tf.Tensor: shape=(50,), dtype=int32, numpy=\narray([-100,    0,    0,    0,    0,    0,    0,    1,    0,    9,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,   15,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    9,    0, -100,\n       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n       -100, -100, -100, -100, -100, -100], dtype=int32)>}\n","output_type":"stream"}]},{"cell_type":"code","source":"def fit_model(train_data, val_data, epochs=2, eta=1e-4, early_stopping_patience=1, batch_size=BATCH_SIZE):\n    model = TFAutoModelForTokenClassification.from_pretrained(CHECKPOINT, num_labels=N_LABELS)\n    learning_schedule = PolynomialDecay(initial_learning_rate=eta, decay_steps=len(train_data) * epochs, end_learning_rate=0)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_schedule))\n\n    print(model.summary())\n    early_stop = EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, mode=\"min\")\n    model.fit(train_data.shuffle(len(train_data)).batch(batch_size), validation_data=val_data.shuffle(len(val_data)).batch(batch_size), \n          epochs=epochs, callbacks=[early_stop])\n    return model","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:51:22.892860Z","iopub.execute_input":"2023-11-11T13:51:22.893101Z","iopub.status.idle":"2023-11-11T13:51:22.900110Z","shell.execute_reply.started":"2023-11-11T13:51:22.893079Z","shell.execute_reply":"2023-11-11T13:51:22.899252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = fit_model(train_data=train_tf_data, val_data=test_tf_data, epochs=25, early_stopping_patience=2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-11-11T13:51:22.901217Z","iopub.execute_input":"2023-11-11T13:51:22.901507Z","iopub.status.idle":"2023-11-11T14:01:52.802757Z","shell.execute_reply.started":"2023-11-11T13:51:22.901484Z","shell.execute_reply":"2023-11-11T14:01:52.801807Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3994ac59e54da9b3e76d57c9037999"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"tf_distil_bert_for_token_classification\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n dropout_19 (Dropout)        multiple                  0         \n                                                                 \n classifier (Dense)          multiple                  13842     \n                                                                 \n=================================================================\nTotal params: 66376722 (253.21 MB)\nTrainable params: 66376722 (253.21 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/25\n1274/1274 [==============================] - 196s 132ms/step - loss: 0.1847 - val_loss: 0.1406\nEpoch 2/25\n1274/1274 [==============================] - 143s 112ms/step - loss: 0.1082 - val_loss: 0.1304\nEpoch 3/25\n1274/1274 [==============================] - 143s 112ms/step - loss: 0.0783 - val_loss: 0.1389\nEpoch 4/25\n1274/1274 [==============================] - 143s 112ms/step - loss: 0.0573 - val_loss: 0.1582\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### INFERENCE","metadata":{}},{"cell_type":"code","source":"def process_output(res):\n    '''\n    Function to concatenate sub-word tokens and labels\n    '''\n    d = {}\n    result = []\n    res.append(['-', 'B-b'])\n    for n, i in enumerate(res):\n        try:\n            split = i[1].split('-')\n            token = i[0]\n            prefix, suffix = split\n            if prefix == 'B':\n                if len(d) != 0:\n                    result.append([(token.replace(\" ##\", \"\"), label) for label, token in d.items()][0])\n                d = {}\n                d[suffix] = token\n            else:\n                d[suffix] = d[suffix] + ' ' + token\n        except:\n            continue\n            \n    return result\n\n\ndef inference(txt):\n    '''\n    Function that returns model prediction\n    '''\n    test_data = [txt]\n    tokenizer = DistilBertTokenizerFast.from_pretrained(CHECKPOINT)\n    tokens = tokenizer.tokenize(txt)\n    tokenized_data = tokenizer(test_data, is_split_into_words=True, max_length=N_TOKENS, \n                               truncation=True, padding=\"max_length\")\n    token_idx_to_consider = tokenized_data.word_ids()\n\n    token_idx_to_consider = [i for i in range(len(token_idx_to_consider)) if token_idx_to_consider[i] is not None] \n    \n    input_ = [tokenized_data['input_ids'], tokenized_data['attention_mask']]\n    pred_logits = model.predict(input_, verbose=0).logits\n    pred = tf.argmax(pred_logits, axis=-1)[0].numpy()\n    pred = pred[token_idx_to_consider]\n    pred_labels = label_encoder.inverse_transform(pd.Series(pred))\n    result = [[token, label] for token, label in zip(tokens, pred_labels) if label.find('-') >= 0]\n    output = process_output(result)\n    return output","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:19:02.126762Z","iopub.execute_input":"2023-11-11T16:19:02.127129Z","iopub.status.idle":"2023-11-11T16:19:02.138600Z","shell.execute_reply.started":"2023-11-11T16:19:02.127097Z","shell.execute_reply":"2023-11-11T16:19:02.137642Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"txt = '''\nOn October 9, the Israeli Defense Minister ordered a \"complete siege\" of Gaza\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:11:29.294182Z","iopub.execute_input":"2023-11-11T16:11:29.294886Z","iopub.status.idle":"2023-11-11T16:11:29.495252Z","shell.execute_reply.started":"2023-11-11T16:11:29.294856Z","shell.execute_reply":"2023-11-11T16:11:29.494318Z"},"trusted":true},"execution_count":315,"outputs":[{"name":"stdout","text":"[('october 9', 'tim'), ('israeli', 'gpe'), ('gaza', 'geo')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nVenezuela issues arrest warrant for US-based opposition leader Juan Guaido\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:11:30.665543Z","iopub.execute_input":"2023-11-11T16:11:30.666186Z","iopub.status.idle":"2023-11-11T16:11:30.872051Z","shell.execute_reply.started":"2023-11-11T16:11:30.666154Z","shell.execute_reply":"2023-11-11T16:11:30.871119Z"},"trusted":true},"execution_count":316,"outputs":[{"name":"stdout","text":"[('venezuela', 'geo'), ('juan guaido', 'per')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nArgentina presidential election heading to run-off with Massa leading Milei\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:11:31.916288Z","iopub.execute_input":"2023-11-11T16:11:31.916666Z","iopub.status.idle":"2023-11-11T16:11:32.127344Z","shell.execute_reply.started":"2023-11-11T16:11:31.916613Z","shell.execute_reply":"2023-11-11T16:11:32.126445Z"},"trusted":true},"execution_count":317,"outputs":[{"name":"stdout","text":"[('argentina', 'geo'), ('massa', 'per'), ('mile', 'per')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nUN Security Council approves sending foreign forces to Haiti\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:12:42.388127Z","iopub.execute_input":"2023-11-11T16:12:42.388372Z","iopub.status.idle":"2023-11-11T16:12:42.599375Z","shell.execute_reply.started":"2023-11-11T16:12:42.388350Z","shell.execute_reply":"2023-11-11T16:12:42.598351Z"},"trusted":true},"execution_count":319,"outputs":[{"name":"stdout","text":"[('un security council', 'org'), ('haiti', 'geo')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nMoody’s sends a warning to America: Your last AAA credit rating is at risk\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:14:13.930457Z","iopub.execute_input":"2023-11-11T16:14:13.930818Z","iopub.status.idle":"2023-11-11T16:14:14.151498Z","shell.execute_reply.started":"2023-11-11T16:14:13.930788Z","shell.execute_reply":"2023-11-11T16:14:14.150582Z"},"trusted":true},"execution_count":322,"outputs":[{"name":"stdout","text":"[('moody ’ s', 'org'), ('america', 'geo'), ('aaa', 'org')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nHaris Rauf Breaks Record For Conceding Most Runs In History Of Cricket World Cup\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:15:35.006430Z","iopub.execute_input":"2023-11-11T16:15:35.007315Z","iopub.status.idle":"2023-11-11T16:15:35.217654Z","shell.execute_reply.started":"2023-11-11T16:15:35.007281Z","shell.execute_reply":"2023-11-11T16:15:35.216545Z"},"trusted":true},"execution_count":323,"outputs":[{"name":"stdout","text":"[('haris rauf', 'per'), ('cricket', 'eve'), ('world cup', 'eve')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nNorth and South American markets finished broadly higher on Friday with shares in U.S. leading the region. \nThe S&P 500 is up 1.56% while Brazil's Bovespa is up 1.29% and Mexico's IPC is up 0.37%.\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:19:05.876013Z","iopub.execute_input":"2023-11-11T16:19:05.876744Z","iopub.status.idle":"2023-11-11T16:19:06.088484Z","shell.execute_reply.started":"2023-11-11T16:19:05.876712Z","shell.execute_reply":"2023-11-11T16:19:06.087479Z"},"trusted":true},"execution_count":330,"outputs":[{"name":"stdout","text":"[('american', 'gpe'), ('friday', 'tim'), ('u . s .', 'geo'), ('s & p', 'org'), ('brazil', 'geo'), ('bovespa', 'geo'), ('mexico', 'geo')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nGoogle, Lendlease axe plans for $15 billion development in Bay Area\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:21:26.780782Z","iopub.execute_input":"2023-11-11T16:21:26.781675Z","iopub.status.idle":"2023-11-11T16:21:26.987467Z","shell.execute_reply.started":"2023-11-11T16:21:26.781640Z","shell.execute_reply":"2023-11-11T16:21:26.986563Z"},"trusted":true},"execution_count":332,"outputs":[{"name":"stdout","text":"[('google', 'org'), ('lendlease axe', 'org'), ('bay area', 'geo')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nAmerican minimum wage has been $7.25 since 2009. What that means for the economy\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:29:19.106148Z","iopub.execute_input":"2023-11-11T16:29:19.106580Z","iopub.status.idle":"2023-11-11T16:29:19.324753Z","shell.execute_reply.started":"2023-11-11T16:29:19.106539Z","shell.execute_reply":"2023-11-11T16:29:19.323701Z"},"trusted":true},"execution_count":338,"outputs":[{"name":"stdout","text":"[('american', 'gpe'), ('2009', 'tim')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nHedge fund billionaire Leon Cooperman, in rare public rebuke of a Republican candidate, says Trump ‘belongs in jail’\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:30:34.696925Z","iopub.execute_input":"2023-11-11T16:30:34.697301Z","iopub.status.idle":"2023-11-11T16:30:34.906960Z","shell.execute_reply.started":"2023-11-11T16:30:34.697273Z","shell.execute_reply":"2023-11-11T16:30:34.906003Z"},"trusted":true},"execution_count":340,"outputs":[{"name":"stdout","text":"[('hedge fund', 'org'), ('leon cooperman', 'per'), ('trump', 'per')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nMore than 1,600 Jewish Harvard alumni threaten to withdraw donations over antisemitism concerns\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:31:03.388004Z","iopub.execute_input":"2023-11-11T16:31:03.388368Z","iopub.status.idle":"2023-11-11T16:31:03.591552Z","shell.execute_reply.started":"2023-11-11T16:31:03.388340Z","shell.execute_reply":"2023-11-11T16:31:03.590663Z"},"trusted":true},"execution_count":341,"outputs":[{"name":"stdout","text":"[('harvard', 'org')]\n","output_type":"stream"}]},{"cell_type":"code","source":"txt = '''\nIsa Soares speaks to David Culver about expectations for next week's meeting \nbetween American President Biden and Chinese President Xi Jinping.\n'''\nprint(inference(txt))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:32:57.814166Z","iopub.execute_input":"2023-11-11T16:32:57.814526Z","iopub.status.idle":"2023-11-11T16:32:58.032337Z","shell.execute_reply.started":"2023-11-11T16:32:57.814496Z","shell.execute_reply":"2023-11-11T16:32:58.031488Z"},"trusted":true},"execution_count":346,"outputs":[{"name":"stdout","text":"[('isa soares', 'per'), ('david culver', 'per'), ('american', 'gpe'), ('president biden', 'per'), ('chinese', 'gpe'), ('president xi jinping', 'per')]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}